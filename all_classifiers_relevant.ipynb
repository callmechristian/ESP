{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AdaBoostClassifier', AdaBoostClassifier()), ('BaggingClassifier', BaggingClassifier()), ('ExtraTreesClassifier', ExtraTreesClassifier()), ('GaussianNB', GaussianNB()), ('GradientBoostingClassifier', GradientBoostingClassifier()), ('HistGradientBoostingClassifier', HistGradientBoostingClassifier()), ('QuadraticDiscriminantAnalysis', QuadraticDiscriminantAnalysis()), ('RandomForestClassifier', RandomForestClassifier())]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "def getClassifiers():\n",
    "    all_clfs_names = []\n",
    "    all_clfs_clf = []\n",
    "    for name, ClassifierClass in estimators:\n",
    "        try:\n",
    "            clf = ClassifierClass()\n",
    "            all_clfs_names.append(name)\n",
    "            all_clfs_clf.append(clf)\n",
    "\n",
    "            # develop\n",
    "            # print(\"{'name':'\" + name + \"', 'params': [[{}],[{}]]},\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            # print('Unable to import', name)\n",
    "            # print(e)\n",
    "    return all_clfs_names, all_clfs_clf\n",
    "\n",
    "df = pd.read_csv(r'data/data_processed.csv')\n",
    "df_continous = pd.read_csv(r'data/data_continous.csv')\n",
    "df_categorical = pd.read_csv(r'data/data_categorical.csv')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# copying job satisfaction into new array\n",
    "JSat = df['JobSatisfaction'].values\n",
    "\n",
    "# trying only binary classification\n",
    "# any value of satisfaction greater than two will be \"Satisfied\" and anything less or equal will be \"Unsatisfied\"\n",
    "JSat_threshold = 2\n",
    "\n",
    "JSat_binary = [0 if val <= JSat_threshold else val for val in JSat]\n",
    "JSat_binary = [1 if val > JSat_threshold else val for val in JSat_binary]\n",
    "# print(JSat_binary)\n",
    "\n",
    "# copying attrition into new column\n",
    "JAtt = df[\"Attrition\"].values\n",
    "\n",
    "# Sivas Code -------------------------------------------------------------\n",
    "# droplist = ['BusinessTravel','DailyRate','EmployeeNumber','HourlyRate','MonthlyRate','NumCompaniesWorked','Over18','StandardHours','TrainingTimesLastYear', 'Attrition']\n",
    "# for val in droplist:\n",
    "#     if val in df.columns.values:\n",
    "#         df.drop(val, axis=1, inplace=True)\n",
    "#         print(val + \" dropped from the dataset successfully.\")\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# splitting inputs by row index\n",
    "df = pd.read_csv(r'data/data_processed.csv')\n",
    "df_cat = pd.read_csv(r'data/data_categorical.csv')\n",
    "\n",
    "df_good = pd.concat([df_cat, df[\"MonthlyIncome\"], df[\"BusinessTravel\"], df[\"StockOptionLevel\"], df[\"DistanceFromHome\"]], axis=1)\n",
    "\n",
    "# print(df_good)\n",
    "# copy attrition and drop from dataframe\n",
    "JAtt = df[\"Attrition\"].values\n",
    "\n",
    "df_good = df_good.drop([\"Unnamed: 0\"], axis=1)\n",
    "df_good = df_good.drop(['Attrition'], axis=1)\n",
    "# print(df_good)\n",
    "# splitting outputs by number\n",
    "JSat_training = JSat[:1200]\n",
    "JSat_validation = JSat[1200:]\n",
    "\n",
    "# splitting new df\n",
    "df_train = df_good.iloc[:1200,:]\n",
    "df_val = df_good.iloc[1200:,:]\n",
    "# splitting Jatt into new sizes\n",
    "JAtt_train = JAtt[:1200]\n",
    "JAtt_val = JAtt[1200:]\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "all_clfs_names, all_clfs_clf = getClassifiers()\n",
    "estimators_trimmed = zip(all_clfs_names, all_clfs_clf)\n",
    "\n",
    "# remove RidgeClassifierCV because it's included in RidgeClassifier with this script\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"RidgeClassifierCV\"]\n",
    "# remove because it should only be used with ensemble methods and is out of scope rn\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"ExtraTreeClassifier\"]\n",
    "\n",
    "# remove these because they have poor performance on our dataset\n",
    "for name in [\"BernoulliNB\" , \"CalibratedClassifierCV\" , \"CategoricalNB\" , \"ComplementNB\" , \"DecisionTreeClassifier\" , \"DummyClassifier\" , \"GaussianProcessClassifier\" , \"KNeighborsClassifier\" ,  \"LabelPropagation\" , \"LabelSpreading\" , \"LinearDiscriminantAnalysis\" , \"LinearSVC\" , \"LogisticRegression\" , \"LogisticRegressionCV\" , \"MLPClassifier\" , \"MultinomialNB\" , \"NearestCentroid\" , \"NuSVC\" , \"PassiveAggressiveClassifier\" , \"Perceptron\" , \"RadiusNeighborsClassifier\" , \"RidgeClassifier\" , \"SGDClassifier\", \"SVC\"]:\n",
    "    estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != name]\n",
    "\n",
    "print(str(estimators_trimmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define possible parameters for all models, this is the mother of config files :)\n",
    "all_possible_model_params = [\n",
    "{'name':'AdaBoostClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"random_state\":None}],[{\"learning_rate\":0.1}, {\"learning_rate\":1}, {\"learning_rate\":10}, {\"learning_rate\":100}]]}, # can implement estimator = any decisiion tree classifier with params... default = DecisionTreeClassifier(max_depth=1)\n",
    "{'name':'BaggingClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}, {\"n_estimators\":10}, {\"n_estimators\":5}],[{\"max_samples\": 1.0},{\"max_samples\": 2.0},{\"max_samples\": 0.1},{\"max_samples\": 0.5},{\"max_samples\": 1},{\"max_samples\": 10}],[{\"max_features\": 1.0},{\"max_features\": 2.0},{\"max_features\": 2.5},{\"max_features\": 10}],[{\"bootstrap\": False}, {\"bootstrap\": True}]]}, # same as above\n",
    "{'name':'ExtraTreesClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 10}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}, {\"max_features\": None}], [{\"min_samples_leaf\": 1}, {\"min_samples_leaf\": 3}, {\"min_samples_leaf\": 5}, {\"min_samples_leaf\": 10}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 3}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": \"balanced_subsample\"}]]},\n",
    "{'name':'GaussianNB', 'params': [[{\"var_smoothing\": 1e-9}, {\"var_smoothing\": 1e-8}, {\"var_smoothing\": 1e-10}],[{\"priors\": None}]]},\n",
    "{'name':'GradientBoostingClassifier', 'params': [[{\"loss\": \"log_loss\"}, {\"loss\": \"exponential\"}],[{\"n_estimators\": 100}, {\"n_estimators\": 10}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e5}, {\"n_estimators\": 1e10}, {\"n_estimators\": 5}], [{\"subsample\": 1}, {\"subsample\": 0.9}, {\"subsample\": 0.5}, {\"subsample\": 0.3}],[{\"min_samples_split\": 2}, {\"min_samples_split\": 4}, {\"min_samples_split\": 8}], [{\"max_depth\": None}, {\"max_depth\": 3}], [{\"max_features\": \"auto\"}, {\"max_features\": None}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 10}, {\"ccp_alpha\": 10}]]},\n",
    "{'name':'HistGradientBoostingClassifier', 'params': [[{\"max_iter\": 100}, {\"max_iter\": 1000}, {\"max_iter\": 500}],[{\"max_leaf_nodes\": 31}, {\"max_leaf_nodes\": None}], [{\"min_samples_leaf\": 20}, {\"min_samples_leaf\": 10}], [{\"max_bins\": 255}, {\"max_bins\": 200}, {\"max_bins\": 150}, {\"max_bins\": 75}]]},\n",
    "{'name':'QuadraticDiscriminantAnalysis', 'params': [[{\"reg_param\": 0.1}, {\"reg_param\": 0.2}, {\"reg_param\": 0.3}, {\"reg_param\": 0.4}, {\"reg_param\": 0.5}],[{\"tol\": 1e-4}, {\"tol\": 1e-3}, {\"tol\": 1e-6}]]},\n",
    "{'name':'RandomForestClassifier', 'params': [[{\"n_estimators\": 100}, {\"n_estimators\": 200}, {\"n_estimators\": 400}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e6}, {\"n_estimators\": 10}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 1}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 0.1}, {\"ccp_alpha\": 0.01}, {\"ccp_alpha\": 1e-3}]]},\n",
    "# {'name':'SVC', 'params': [[{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}],[{\"kernel\":\"linear\"}, {\"kernel\":\"rbf\"}, {\"kernel\":\"sigmoid\"}, {\"kernel\":\"poly\"}], [{\"degree\":1}, {\"degree\":2}, {\"degree\":3}, {\"degree\":4}]]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8958\n"
     ]
    }
   ],
   "source": [
    "# pair all possible parameters of a type with others of a different types... basically combinations of n by k from the parameters\n",
    "import itertools\n",
    "\n",
    "# count models for stats\n",
    "total_nr_models = 0\n",
    "# store params list by model\n",
    "params_by_model = []\n",
    "\n",
    "for model_family in all_possible_model_params:\n",
    "    params = model_family[\"params\"]\n",
    "\n",
    "    # initialize an empty param dict for model\n",
    "    param_dict = []\n",
    "    # combinations of n by k from unique available params\n",
    "    combinations = [p for p in itertools.product(*params)]\n",
    "    # concatenate each tuple of unique params into a dict and save into param_dict\n",
    "    for combination in combinations:\n",
    "        param_dict.append(dict(itertools.chain.from_iterable(d.items() for d in combination)))\n",
    "\n",
    "    # count models\n",
    "    total_nr_models = total_nr_models + len(param_dict)\n",
    "\n",
    "    # append param list asssociated with model name\n",
    "    params_by_model.append({\"name\": model_family[\"name\"], \"params\": param_dict})\n",
    "    \n",
    "    # debug\n",
    "    # print(param_dict)\n",
    "    # break\n",
    "\n",
    "print(total_nr_models) # f*ck that's a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(params_by_model))\n",
    "model_classes = []\n",
    "i = 0\n",
    "\n",
    "for clf_name, clf in estimators_trimmed:\n",
    "    \n",
    "    # some errors without, unknown casting bug ?\n",
    "    clf_name = str(clf_name)\n",
    "\n",
    "    # make sure we are not passing the wrong parameters to the classifier\n",
    "    if params_by_model[i][\"name\"] == clf_name:\n",
    "        clf_params = params_by_model[i][\"params\"]\n",
    "    else:\n",
    "        print(\"Something went wrong! - Tried passing params from \" + params_by_model[i][\"name\"] + \" to \" + clf_name + \"!\\n\")\n",
    "\n",
    "    model_classes.append([clf, clf_name, clf_params])\n",
    "    # debug\n",
    "    # print(\"Appended \" + str(clf_name) + \" with params \" + str(clf_params) + \"to object \" + str(clf))\n",
    "    # break\n",
    "    \n",
    "    # increment our index for the parameter lookup\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RandomForestClassifier with params: {'n_estimators': 10, 'criterion': 'log_loss', 'min_samples_split': 1, 'max_features': 'log2', 'bootstrap': False, 'n_jobs': -1, 'ccp_alpha': 0.001}... 8957 OUT OF 8958\n",
      " failed in 0.03798413276672363s;\n",
      "\n",
      "min_samples_split == 1, must be >= 2.\n",
      "\n",
      "Finalized all trainings in 2229.2598531246185s. Phew!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "# so we can hopefully clear some output cell from overflowing...\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# let's get some statistics about this process\n",
    "global_stats = {\n",
    "    \"trained_models\": 0,\n",
    "    \"failed_models\": 0,\n",
    "    \"total_models\": total_nr_models,\n",
    "    \"training_time\": 0,\n",
    "    \"total_training_time\": 0\n",
    "}\n",
    "\n",
    "model_stats = []\n",
    "\n",
    "# let's also time the execution of these, and total execution :)\n",
    "# total time start\n",
    "t_start = time.time()\n",
    "# var for longest model train time\n",
    "longest_total_model_train_time = 0\n",
    "\n",
    "insights = []\n",
    "for Model, modelname, params_list in model_classes:\n",
    "\n",
    "    # check that params are initialized\n",
    "    if params_list != []:\n",
    "\n",
    "        # model timer start\n",
    "        t_start_model = time.time()\n",
    "        # longest param train time\n",
    "        longest_param_train_time = 0\n",
    "\n",
    "        for params in params_list:\n",
    "            # param timer start\n",
    "            t_start_param = time.time()\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            print(\"Starting \" + str(modelname) + \" with params: \" + str(params) + \"... \" +\n",
    "                  str(global_stats[\"trained_models\"] + global_stats[\"failed_models\"]) + \" OUT OF \" + str(global_stats[\"total_models\"]))\n",
    "\n",
    "            # define insight vars in case of failure\n",
    "            model_accuracy = 0\n",
    "            model_log_loss = 0\n",
    "            model_precision = 0\n",
    "            model_recall = 0\n",
    "            model_f1 = 0\n",
    "\n",
    "            # in case something goes wrong or unsupported params\n",
    "            try:\n",
    "                # some models fail when called with args ? unknown bug\n",
    "                model = Model\n",
    "                # some models fail with *kwargs\n",
    "                model.set_params(**params)\n",
    "                model.fit(df_train, JAtt_train)\n",
    "\n",
    "                JAtt_predicted = model.predict(df_val)\n",
    "\n",
    "                # compute model accuracy\n",
    "                model_accuracy = metrics.accuracy_score(\n",
    "                    JAtt_val, JAtt_predicted)\n",
    "                # compute model log loss\n",
    "                # model_log_loss = metrics.log_loss(JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                # compute model precision\n",
    "                model_precision = metrics.precision_score(\n",
    "                    JAtt_val, JAtt_predicted, average=\"micro\")\n",
    "                #  compute model precision\n",
    "                model_recall = metrics.recall_score(\n",
    "                    JAtt_val, JAtt_predicted, average=\"micro\")\n",
    "                # compute model f1 score\n",
    "                model_f1 = 2*(model_precision*model_recall) / \\\n",
    "                    (model_precision + model_recall)\n",
    "\n",
    "                # param timer end\n",
    "                t_end_param = time.time()\n",
    "                # compute elapsed\n",
    "                param_time = t_end_param - t_start_param\n",
    "\n",
    "                print(\" finished in \" + str(param_time) +\n",
    "                      \"s; had an f1 of: \" + str(model_f1) + \"\\n\")\n",
    "                global_stats[\"trained_models\"] += 1\n",
    "            except Exception as e:\n",
    "                print(\" failed in \" + str(param_time) + \"s;\\n\")\n",
    "                print(e)\n",
    "                global_stats[\"failed_models\"] += 1\n",
    "\n",
    "            if longest_param_train_time < param_time:\n",
    "                longest_param_train_time = param_time\n",
    "\n",
    "            insights.append((modelname, model, params, model_accuracy, model_f1,\n",
    "                            model_log_loss, model_precision, model_recall, param_time))\n",
    "\n",
    "        # model timer end\n",
    "        t_end_model = time.time()\n",
    "        # get total model train time\n",
    "        model_time = t_end_model - t_start_model\n",
    "        # compare longest total model train time\n",
    "        if longest_total_model_train_time < model_time:\n",
    "            longest_total_model_train_time = model_time\n",
    "\n",
    "        # let's get stats by model\n",
    "        model_stats.append({\n",
    "            \"name\": modelname,\n",
    "            \"tuned_params\": len(params),\n",
    "            \"variations_tested\": len(params_list),\n",
    "            \"total_train_time\": t_end_model - t_start_model,\n",
    "            \"longest_train_time\": longest_param_train_time,\n",
    "        })\n",
    "\n",
    "# total time end\n",
    "t_end = time.time()\n",
    "# compute total time\n",
    "t_total = t_end - t_start\n",
    "\n",
    "global_stats[\"total_training_time\"] = t_total\n",
    "\n",
    "print(\"\\nFinalized all trainings in \" + str(t_total) + \"s. Phew!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 24-01-2023-01-12-03\n"
     ]
    }
   ],
   "source": [
    "# in case I forgot something (and damn it I did... so many times...) and because the computation time is long I don't want to waste all the computation that I already did, so let's save it into a log file\n",
    "# maybe I should've spent more time on a way to cache the models... not sure I have enough space for that. Hey! Guess I should've used google collab...\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# datetime object containing current date and time\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n",
    "# save current insights into a file\n",
    "with open(\"out/logs/insights-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(insights, f)\n",
    "with open(\"out/logs/globalstats-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(global_stats, f)\n",
    "with open(\"out/logs/modelstats-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(model_stats, f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "# ^^^^ prevents this cell from being executed automatically\n",
    "# load an insights log file\n",
    "load_file_name = \"insights-23-01-2023-20-16-17\" + \".pck\"\n",
    "with open(load_file_name, 'rb') as f:\n",
    "    insights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize plots by different model parameters\n",
    "plt_models_by_clf = {}\n",
    "\n",
    "# reformat insights data to easier fit our plot requirements\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    # check if key exists and if not add new dict key with model type (name)\n",
    "    if not model_name in plt_models_by_clf:\n",
    "        plt_models_by_clf.update({str(model_name): []})\n",
    "    \n",
    "    plt_models_by_clf[model_name].append({\n",
    "        \"model_class\": model_class,\n",
    "        \"model_params\": model_params,\n",
    "        \"model_accuracy\": model_accuracy,\n",
    "        \"model_f1\": model_f1,\n",
    "        \"model_fbeta\": model_fbeta,\n",
    "        \"model_log_loss\": model_log_loss,\n",
    "        \"model_precision\": model_precision,\n",
    "        \"model_time\": model_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best accuracy models and compare them to the rest\n",
    "plt_model_best = []\n",
    "_count = 0\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    mdl_found = False\n",
    "    for mdl in plt_model_best:\n",
    "        if mdl[\"name\"] == model_name:\n",
    "            mdl_found = True\n",
    "    \n",
    "    if not mdl_found:\n",
    "        plt_model_best.append({\"name\": model_name, \"model_class\": model_class, \"parameters\": model_params, \"f1_score\": model_f1, \"model_time\": model_time})\n",
    "    else:\n",
    "        for best_name, _, best_parameters, best_f1, best_time in plt_model_best:\n",
    "            _count = _count + 1\n",
    "            if best_name == model_name:\n",
    "                print(best_name + \" == \" + model_name)\n",
    "                if best_f1 < model_f1:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"f1_score\"] = model_f1\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                    plt_model_best[_count][\"model_class\"] = model_class\n",
    "                if best_f1 == model_f1 and best_time == model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = best_parameters + model_params\n",
    "                if best_f1 == model_f1 and best_time > model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                    plt_model_best[_count][\"model_class\"] = model_class\n",
    "                \n",
    "# print(str(plt_model_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AdaBoostClassifier with best f1 score 0.837037037037037 and it took 0.3s to train\n",
      "Model BaggingClassifier with best f1 score 0.7037037037037037 and it took 0.9s to train\n",
      "Model ExtraTreesClassifier with best f1 score 0.837037037037037 and it took 0.29s to train\n",
      "Model GaussianNB with best f1 score 0.8407407407407409 and it took 0.0s to train\n",
      "Model GradientBoostingClassifier with best f1 score 0.7296296296296296 and it took 0.91s to train\n",
      "Model HistGradientBoostingClassifier with best f1 score 0.8481481481481481 and it took 0.9s to train\n",
      "Model QuadraticDiscriminantAnalysis with best f1 score 0.8444444444444444 and it took 0.01s to train\n",
      "Model RandomForestClassifier with best f1 score 0.8518518518518519 and it took 0.31s to train\n"
     ]
    }
   ],
   "source": [
    "# print all models\n",
    "for model in plt_model_best:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForestClassifier with best f1 score 0.8518518518518519 and it took 0.31s to train\n"
     ]
    }
   ],
   "source": [
    "# print models with best f1 score\n",
    "best_f1 = 0\n",
    "best_models = []\n",
    "\n",
    "for model in plt_model_best:\n",
    "    if model[\"f1_score\"] > best_f1:\n",
    "        best_models = [model]\n",
    "    else:\n",
    "        best_models.append(model)\n",
    "        best_f1 = model[\"f1_score\"]\n",
    "\n",
    "for model in best_models:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'RandomForestClassifier', 'model_class': RandomForestClassifier(bootstrap=False, ccp_alpha=0.001, criterion='log_loss',\n",
      "                       max_features='log2', min_samples_split=1,\n",
      "                       n_estimators=10, n_jobs=-1), 'parameters': {'n_estimators': 100, 'criterion': 'gini', 'min_samples_split': 2, 'max_features': 'sqrt', 'bootstrap': True, 'n_jobs': -1, 'ccp_alpha': 0}, 'f1_score': 0.8518518518518519, 'model_time': 0.3071432113647461}\n"
     ]
    }
   ],
   "source": [
    "# print model with best f1 and train time\n",
    "best_time = 1000\n",
    "best_model = None\n",
    "\n",
    "for model in best_models:\n",
    "    if model[\"model_time\"] < best_time:\n",
    "        best_model = model\n",
    "        best_time = model[\"model_time\"]\n",
    "\n",
    "print(str(best_model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6a3af7cc3dd87b1c9aec9c941132942751acefef11dd989f38e57e7ca82ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
