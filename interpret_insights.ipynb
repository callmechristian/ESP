{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load an insights log file\n",
    "load_file_name = \"out/logs/insights-24-01-2023-14-40-53\" + \".pck\"\n",
    "with open(load_file_name, 'rb') as f:\n",
    "    insights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize plots by different model parameters\n",
    "plt_models_by_clf = {}\n",
    "\n",
    "# reformat insights data to easier fit our plot requirements\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    # check if key exists and if not add new dict key with model type (name)\n",
    "    if not model_name in plt_models_by_clf:\n",
    "        plt_models_by_clf.update({str(model_name): []})\n",
    "    \n",
    "    plt_models_by_clf[model_name].append({\n",
    "        \"model_class\": model_class,\n",
    "        \"model_params\": model_params,\n",
    "        \"model_accuracy\": model_accuracy,\n",
    "        \"model_f1\": model_f1,\n",
    "        \"model_fbeta\": model_fbeta,\n",
    "        \"model_log_loss\": model_log_loss,\n",
    "        \"model_precision\": model_precision,\n",
    "        \"model_time\": model_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best accuracy models and compare them to the rest\n",
    "plt_model_best = []\n",
    "_count = 0\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    mdl_found = False\n",
    "    for mdl in plt_model_best:\n",
    "        if mdl[\"name\"] == model_name:\n",
    "            mdl_found = True\n",
    "    \n",
    "    if not mdl_found:\n",
    "        plt_model_best.append({\"name\": model_name, \"parameters\": model_params, \"f1_score\": model_f1, \"model_time\": model_time})\n",
    "    else:\n",
    "        for best_name, best_parameters, best_f1, best_time in plt_model_best:\n",
    "            _count = _count + 1\n",
    "            if best_name == model_name:\n",
    "                print(best_name + \" == \" + model_name)\n",
    "                if best_f1 < model_f1:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"f1_score\"] = model_f1\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                if best_f1 == model_f1 and best_time == model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = best_parameters + model_params\n",
    "                if best_f1 == model_f1 and best_time > model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AdaBoostClassifier with best f1 score 0.837037037037037 and it took 0.24s to train\n",
      "Model BaggingClassifier with best f1 score 0.7111111111111111 and it took 0.7s to train\n",
      "Model BernoulliNB with best f1 score 0.8185185185185185 and it took 0.02s to train\n",
      "Model CalibratedClassifierCV with best f1 score 0.8185185185185185 and it took 0.27s to train\n",
      "Model CategoricalNB with best f1 score 0 and it took 0.64s to train\n",
      "Model ComplementNB with best f1 score 0.8222222222222222 and it took 0.01s to train\n",
      "Model DecisionTreeClassifier with best f1 score 0 and it took 0.01s to train\n",
      "Model DummyClassifier with best f1 score 0.8185185185185185 and it took 0.0s to train\n",
      "Model ExtraTreesClassifier with best f1 score 0.8407407407407409 and it took 0.25s to train\n",
      "Model GaussianNB with best f1 score 0.8407407407407409 and it took 0.0s to train\n",
      "Model GaussianProcessClassifier with best f1 score 0.7518518518518519 and it took 0.76s to train\n",
      "Model GradientBoostingClassifier with best f1 score 0.7296296296296296 and it took 1.1s to train\n",
      "Model HistGradientBoostingClassifier with best f1 score 0.8481481481481481 and it took 0.94s to train\n",
      "Model KNeighborsClassifier with best f1 score 0.7666666666666667 and it took 0.05s to train\n",
      "Model LabelPropagation with best f1 score 0 and it took 0.06s to train\n",
      "Model LabelSpreading with best f1 score 0 and it took 0.12s to train\n",
      "Model LinearDiscriminantAnalysis with best f1 score 0.8740740740740742 and it took 0.03s to train\n",
      "Model LinearSVC with best f1 score 0.8185185185185185 and it took 0.1s to train\n",
      "Model LogisticRegression with best f1 score 0.6703703703703704 and it took 2.79s to train\n",
      "Model LogisticRegressionCV with best f1 score 0 and it took 1.62s to train\n",
      "Model MLPClassifier with best f1 score 0.8185185185185185 and it took 0.09s to train\n",
      "Model MultinomialNB with best f1 score 0.5703703703703704 and it took 0.01s to train\n",
      "Model NearestCentroid with best f1 score 0.4185185185185185 and it took 0.01s to train\n",
      "Model NuSVC with best f1 score 0 and it took 0.01s to train\n",
      "Model PassiveAggressiveClassifier with best f1 score 0.8185185185185185 and it took 0.02s to train\n",
      "Model Perceptron with best f1 score 0.8185185185185185 and it took 0.01s to train\n",
      "Model QuadraticDiscriminantAnalysis with best f1 score 0.8444444444444444 and it took 0.04s to train\n",
      "Model RadiusNeighborsClassifier with best f1 score 0 and it took 0.02s to train\n",
      "Model RandomForestClassifier with best f1 score 0.8518518518518519 and it took 0.2s to train\n",
      "Model RidgeClassifier with best f1 score 0 and it took 0.05s to train\n",
      "Model SGDClassifier with best f1 score 0.24444444444444444 and it took 0.01s to train\n",
      "Model SVC with best f1 score 0.8185185185185185 and it took 110.51s to train\n"
     ]
    }
   ],
   "source": [
    "# print all models\n",
    "for model in plt_model_best:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AdaBoostClassifier with best f1 score 0.837037037037037 and it took 0.24s to train\n",
      "Model ExtraTreesClassifier with best f1 score 0.8407407407407409 and it took 0.25s to train\n",
      "Model GaussianNB with best f1 score 0.8407407407407409 and it took 0.0s to train\n",
      "Model HistGradientBoostingClassifier with best f1 score 0.8481481481481481 and it took 0.94s to train\n",
      "Model LinearDiscriminantAnalysis with best f1 score 0.8740740740740742 and it took 0.03s to train\n"
     ]
    }
   ],
   "source": [
    "# print some models with best f1 score\n",
    "best_f1 = 0\n",
    "best_models = []\n",
    "\n",
    "for model in plt_model_best:\n",
    "    if model[\"f1_score\"] >= best_f1:\n",
    "        best_models.append(model)\n",
    "        best_f1 = model[\"f1_score\"]\n",
    "\n",
    "for model in best_models:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'GaussianNB', 'parameters': {'var_smoothing': 1e-09, 'priors': None}, 'f1_score': 0.8407407407407409, 'model_time': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# print model with best f1 and train time\n",
    "best_time = 1000\n",
    "best_model = None\n",
    "\n",
    "for model in best_models:\n",
    "    if model[\"model_time\"] < best_time:\n",
    "        best_model = model\n",
    "        best_time = model[\"model_time\"]\n",
    "\n",
    "print(str(best_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6a3af7cc3dd87b1c9aec9c941132942751acefef11dd989f38e57e7ca82ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
