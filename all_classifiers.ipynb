{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "def getClassifiers():\n",
    "    all_clfs_names = []\n",
    "    all_clfs_clf = []\n",
    "    for name, ClassifierClass in estimators:\n",
    "        try:\n",
    "            clf = ClassifierClass()\n",
    "            all_clfs_names.append(name)\n",
    "            all_clfs_clf.append(clf)\n",
    "\n",
    "            # develop\n",
    "            # print(\"{'name':'\" + name + \"', 'params': [[{}],[{}]]},\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            # print('Unable to import', name)\n",
    "            # print(e)\n",
    "    return all_clfs_names, all_clfs_clf\n",
    "\n",
    "df = pd.read_csv(r'data/data_processed.csv')\n",
    "df_continous = pd.read_csv(r'data/data_continous.csv')\n",
    "df_categorical = pd.read_csv(r'data/data_categorical.csv')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# copying job satisfaction into new array\n",
    "JSat = df['JobSatisfaction'].values\n",
    "\n",
    "# trying only binary classification\n",
    "# any value of satisfaction greater than two will be \"Satisfied\" and anything less or equal will be \"Unsatisfied\"\n",
    "JSat_threshold = 2\n",
    "\n",
    "JSat_binary = [0 if val <= JSat_threshold else val for val in JSat]\n",
    "JSat_binary = [1 if val > JSat_threshold else val for val in JSat_binary]\n",
    "print(JSat_binary)\n",
    "\n",
    "# splitting inputs by row index\n",
    "# continous data\n",
    "df_training_continous = df_continous.iloc[:1200,:]\n",
    "df_validation_continous = df_continous.iloc[1200:,:]\n",
    "# categorical data\n",
    "df_training_categorical = df_categorical.iloc[:1200,:]\n",
    "df_validation_categorical = df_categorical.iloc[1200:,:]\n",
    "# splitting outputs by number\n",
    "JSat_training = JSat[:1200]\n",
    "JSat_validation = JSat[1200:]\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "all_clfs_names, all_clfs_clf = getClassifiers()\n",
    "estimators_trimmed = zip(all_clfs_names, all_clfs_clf)\n",
    "\n",
    "# remove RidgeClassifierCV because it's included in RidgeClassifier with this script\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"RidgeClassifierCV\"]\n",
    "# remove because it should only be used with ensemble methods and is out of scope rn\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"ExtraTreeClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define possible parameters for all models, this is the mother of config files :)\n",
    "all_possible_model_params = [\n",
    "{'name':'AdaBoostClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"random_state\":None}],[{\"learning_rate\":0.1}, {\"learning_rate\":1}, {\"learning_rate\":10}, {\"learning_rate\":100}]]}, # can implement estimator = any decisiion tree classifier with params... default = DecisionTreeClassifier(max_depth=1)\n",
    "{'name':'BaggingClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}, {\"n_estimators\":10}, {\"n_estimators\":5}],[{\"max_samples\": 1.0},{\"max_samples\": 2.0},{\"max_samples\": 0.1},{\"max_samples\": 0.5},{\"max_samples\": 1},{\"max_samples\": 10}],[{\"max_features\": 1.0},{\"max_features\": 2.0},{\"max_features\": 2.5},{\"max_features\": 10}],[{\"bootstrap\": False}, {\"bootstrap\": True}]]}, # same as above\n",
    "{'name':'BernoulliNB', 'params': [[{\"alpha\": 1.0}, {\"alpha\": 100.0}, {\"alpha\": 1e-12}, {\"alpha\": 1e-6}, {\"alpha\": 1e-3}], [{\"binarize\": None}, {\"binarize\": 0.0}, {\"binarize\": 1.0}, {\"binarize\": 2.5}, {\"binarize\": 5.0}], [{\"fit_prior\": True}, {\"fit_prior\": False}]]},\n",
    "{'name':'CalibratedClassifierCV', 'params': [[{'method':'sigmoid'}, {'method:':'isotonic'}] ,[{\"cv\": None}, {\"cv\": 1}, {\"cv\": 3}, {\"cv\": 10}]]},\n",
    "{'name':'CategoricalNB', 'params': [[{\"fit_prior\": True}, {\"fit_prior\": False}],[{\"alpha\": 0}, {\"alpha\": 1}, {\"alpha\": 3}, {\"alpha\": 5}]]},\n",
    "{'name':'ComplementNB', 'params': [[{\"fit_prior\": True}, {\"fit_prior\": False}],[{\"norm\": True}, {\"norm\": False}], [{\"alpha\": 0}, {\"alpha\": 1}, {\"alpha\": 3}, {\"alpha\": 5}]]},\n",
    "{'name':'DecisionTreeClassifier', 'params': [[{\"splitter\": \"best\"}, {\"splitter\": \"random\"}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 1}, {\"min_samples_split\": 2}]]},\n",
    "{'name':'DummyClassifier', 'params': [[{\"strategy\": \"prior\"}],[{\"random_state\": None}]]},\n",
    "{'name':'ExtraTreesClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 10}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}, {\"max_features\": None}], [{\"min_samples_leaf\": 1}, {\"min_samples_leaf\": 3}, {\"min_samples_leaf\": 5}, {\"min_samples_leaf\": 10}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 3}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": \"balanced_subsample\"}]]},\n",
    "{'name':'GaussianNB', 'params': [[{\"var_smoothing\": 1e-9}, {\"var_smoothing\": 1e-8}, {\"var_smoothing\": 1e-10}],[{\"priors\": None}]]},\n",
    "{'name':'GaussianProcessClassifier', 'params': [[{\"max_iter_predict\": 100}, {\"max_iter_predict\": 400}, {\"max_iter_predict\": 1000}],[{\"n_jobs\": -1}], [{\"multi_class\": \"one_vs_rest\"}, {\"multi_class\": \"one_vs_one\"}]]},\n",
    "{'name':'GradientBoostingClassifier', 'params': [[{\"loss\": \"log_loss\"}, {\"loss\": \"exponential\"}],[{\"n_estimators\": 100}, {\"n_estimators\": 10}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e5}, {\"n_estimators\": 1e10}, {\"n_estimators\": 5}], [{\"subsample\": 1}, {\"subsample\": 0.9}, {\"subsample\": 0.5}, {\"subsample\": 0.3}],[{\"min_samples_split\": 2}, {\"min_samples_split\": 4}, {\"min_samples_split\": 8}], [{\"max_depth\": None}, {\"max_depth\": 3}], [{\"max_features\": \"auto\"}, {\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}, {\"max_features\": None}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 10}, {\"ccp_alpha\": 10}]]},\n",
    "{'name':'HistGradientBoostingClassifier', 'params': [[{\"max_iter\": 100}, {\"max_iter\": 1000}, {\"max_iter\": 500}],[{\"max_leaf_nodes\": 31}, {\"max_leaf_nodes\": None}], [{\"min_samples_leaf\": 20}, {\"min_samples_leaf\": 10}], [{\"max_bins\": 255}, {\"max_bins\": 200}, {\"max_bins\": 150}, {\"max_bins\": 75}]]},\n",
    "{'name':'KNeighborsClassifier', 'params': [[{\"n_neighbors\": 5}],[{\"weights\": \"uniform\"}, {\"weights\": \"distance\"}], [{\"leaf_size\": 10}, {\"leaf_size\": 20}, {\"leaf_size\": 30}, {\"leaf_size\": 40}, {\"leaf_size\": 5}, {\"leaf_size\": 15}, {\"leaf_size\": 25}, {\"leaf_size\": 35}, {\"leaf_size\": 50}], [{\"p\": 2}, {\"p\": 1}, {\"p\": 3}, {\"p\": 4}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'LabelPropagation', 'params': [[{\"kernel\": \"rnn\"}, {\"kernel\": \"rbf\"}],[{\"gamma\": 20}, {\"gamma\": 10}, {\"gamma\": 5}, {\"gamma\": 40}], [{\"n_neighbors\": 1}, {\"n_neighbors\": 3}, {\"n_neighbors\": 7}, {\"n_neighbors\": 10}, {\"n_neighbors\": 20}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 4000}, {\"max_iter\": 10000}], [{\"n_jobs\": -1}], [{\"tol\": 1e-3}, {\"tol\": 1e-2}, {\"tol\": 1e-4}]]},\n",
    "{'name':'LabelSpreading', 'params': [[{\"kernel\": \"rnn\"}, {\"kernel\": \"rbf\"}],[{\"gamma\": 20}, {\"gamma\": 10}, {\"gamma\": 5}, {\"gamma\": 40}], [{\"n_neighbors\": 1}, {\"n_neighbors\": 3}, {\"n_neighbors\": 7}, {\"n_neighbors\": 10}, {\"n_neighbors\": 20}], [{\"max_iter\": 300}, {\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 30}], [{\"n_jobs\": -1}], [{\"alpha\": 0.2}, {\"alpha\": 0.1}, {\"alpha\": 0}, {\"alpha\": 0.5}, {\"alpha\": 0.9}, {\"alpha\": 1}]]},\n",
    "{'name':'LinearDiscriminantAnalysis', 'params': [[{\"solver\": \"svd\"}, {\"solver\": \"lsqr\"}, {\"solver\": \"eigen\"}],[{\"shrinkage\": None}, {\"shrinkage\": \"auto\"}]]},\n",
    "{'name':'LinearSVC', 'params': [[{\"C\": 1}, {\"C\": 0.5}, {\"C\": 0.9}, {\"C\": 2}],[{\"multi_class\": \"ovr\"}, {\"multi_class\": \"crammer_singer\"}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 4000}, {\"max_iter\": 10000}]]},\n",
    "{'name':'LogisticRegression', 'params': [[{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}],[{\"solver\":'lbfgs'}, {\"solver\":'liblinear'}, {\"solver\":'newton-cg'}, {\"solver\":'sag'}, {\"solver\":'saga'}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"n_jobs\": -1}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 4000}, {\"max_iter\": 10000}]]},\n",
    "{'name':'LogisticRegressionCV', 'params': [[{\"cv\": None}, {\"cv\": 1}, {\"cv\": 3}, {\"cv\": 5}, {\"cv\": 8}, {\"cv\": 1}],[{\"Cs\":0.01}, {\"Cs\":0.1}, {\"Cs\":1}, {\"Cs\":10}],[{\"solver\":'lbfgs'}, {\"solver\":'liblinear'}, {\"solver\":'newton-cg'}, {\"solver\":'sag'}, {\"solver\":'saga'}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"n_jobs\": -1}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 4000}, {\"max_iter\": 10000}]]},\n",
    "{'name':'MLPClassifier', 'params': [[{\"hidden_layer_sizes\": 100}, {\"hidden_layer_sizes\": 35}, {\"hidden_layer_sizes\": 200}, {\"hidden_layer_sizes\": 500}],[{\"activation\": \"identity\"}, {\"activation\": \"logistic\"}, {\"activation\": \"tanh\"}, {\"activation\": \"relu\"}],[{\"solver\": \"lbfgs\"}, {\"solver\": \"sgd\"}, {\"solver\": \"adam\"}], [{\"alpha\": 1e-4}, {\"alpha\": 1e-5}, {\"alpha\": 1e-3}, {\"alpha\": 1e-6}], [{\"learning_rate\": \"constant\"}, {\"learning_rate\": \"invscaling\"}, {\"learning_rate\": \"adaptive\"}], [{\"max_iter\": 200}, {\"max_iter\": 2000}, {\"max_iter\": 4000}, {\"max_iter\": 10000}]]},\n",
    "{'name':'MultinomialNB', 'params': [[{\"alpha\": 0}, {\"alpha\": 1.0}, {\"alpha\": 100.0}, {\"alpha\": 1e-12}, {\"alpha\": 1e-6}, {\"alpha\": 1e-3}],[{\"fit_prior\": True}, {\"fit_prior\": False}]]},\n",
    "{'name':'NearestCentroid', 'params': [[{\"metric\": \"euclidean\"}, {\"metric\": \"manhattan\"}, {\"metric\": \"cosine\"}, {\"metric\": \"haversine\"}, {\"metric\": \"cityblock\"}],[{\"shrink_threshold\": None}, {\"shrink_threshold\": 1e-4}, {\"shrink_threshold\": 1e4}]]},\n",
    "{'name':'NuSVC', 'params': [[{\"C\":1e-4}, {\"C\":1e-3}, {\"C\":1e-2}, {\"C\":0.1}, {\"C\":1}],[{\"kernel\":\"linear\"}, {\"kernel\":\"rbf\"}, {\"kernel\":\"sigmoid\"}, {\"kernel\":\"poly\"}], [{\"degree\":1}, {\"degree\":2}, {\"degree\":3}, {\"degree\":4}]]},\n",
    "{'name':'PassiveAggressiveClassifier', 'params': [[{\"C\":1e-4}, {\"C\":1e-3}, {\"C\":1e-2}, {\"C\":0.1}, {\"C\":1}],[{\"max_iter\": 1000}, {\"max_iter\": -1}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'Perceptron', 'params': [[{\"penalty\": \"l1\"}, {\"penalty\": \"l2\"}, {\"penalty\": \"elasticnet\"}, {\"penalty\": None}],[{\"alpha\": 1e-4}, {\"alpha\": 1e-3}, {\"alpha\": 1e-2}, {\"alpha\": 1e-5}], [{\"l1_ratio\": 0.15}, {\"l1_ratio\": 0.05}, {\"l1_ratio\": 0.25}, {\"l1_ratio\": 0.5}, {\"l1_ratio\": 0.75}]]},\n",
    "{'name':'QuadraticDiscriminantAnalysis', 'params': [[{\"reg_param\": 0.1}, {\"reg_param\": 0.2}, {\"reg_param\": 0.3}, {\"reg_param\": 0.4}, {\"reg_param\": 0.5}],[{\"tol\": 1e-4}, {\"tol\": 1e-3}, {\"tol\": 1e-6}]]},\n",
    "{'name':'RadiusNeighborsClassifier', 'params': [[{\"radius\": 1}, {\"radius\": 2}, {\"radius\": 3}, {\"radius\": 1e-1}, {\"radius\": 1e-2}],[{\"weights\": \"uniform\"}, {\"weights\": \"distance\"}], [{\"algorithm\": \"auto\"}, {\"algorithm\": \"ball_tree\"}, {\"algorithm\": \"kd_tree\"}, {\"algorithm\": \"brute\"}], [{\"p\": 1}, {\"p\": 2}], [{\"metric\": \"euclidean\"}, {\"metric\": \"manhattan\"}, {\"metric\": \"cosine\"}, {\"metric\": \"haversine\"}, {\"metric\": \"cityblock\"}, {\"metric\": \"minkowski\"}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'RandomForestClassifier', 'params': [[{\"n_estimators\": 100}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e6}, {\"n_estimators\": 10}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 1}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}, {\"n_jobs\": None}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 0.1}, {\"ccp_alpha\": 0.01}]]},\n",
    "{'name':'RidgeClassifier', 'params': [[{\"solver\": \"svd\"}, {\"solver\": \"lsqr\"}, {\"solver\": \"cholesky\"}, {\"solver\": \"sparse_cg\"}, {\"solver\": \"sag\"}, {\"solver\": \"saga\"}, {\"solver\": \"lbfgs\"}], [{\"alpha\": 1}, {\"alpha\": 1e-2}, {\"alpha\": 1e-4}, {\"alpha\": 1e2}, {\"alpha\": 1e4}],[{\"positive\": True}, {\"positive\": False}]]},\n",
    "{'name':'SGDClassifier', 'params': [[{\"loss\": \"hinge\"}, {\"loss\": \"log_loss\"}, {\"loss\": \"modified_huber\"}, {\"loss\": \"squared_hinge\"}, {\"loss\": \"perceptron\"}, {\"loss\": \"squared_error\"}, {\"loss\": \"huber\"}, {\"loss\": \"epsilon_insensitive\"}, {\"loss\": \"squared_epsilon_insensitive\"}],[{\"penalty\": \"l1\"}, {\"penalty\": \"l2\"}, {\"penalty\": \"elasticnet\"}, {\"penalty\": None}], [{\"alpha\": 1e-4}, {\"alpha\": 1e-3}, {\"alpha\": 1e-2}, {\"alpha\": 1e-1}, {\"alpha\": 1}, {\"alpha\": 1e2}, {\"alpha\": 1e3}, {\"alpha\": 1e4}], [{\"l1_ratio\": 0.15}, {\"l1_ratio\": 0.05}, {\"l1_ratio\": 0.25}, {\"l1_ratio\": 0.5}, {\"l1_ratio\": 0.75}], [{\"max_iter\": 1e4}, {\"max_iter\": 1e5}, {\"max_iter\": 1e6}], [{\"n_jobs\": None}, {\"n_jobs\": -1}], [{\"epsilon\": 1e-1}, {\"epsilon\": 1}, {\"epsilon\": 1e-2}], [{\"learning_rate\": \"constant\"}, {\"learning_rate\": \"optimal\"}, {\"learning_rate\": \"invscaling\"}, {\"learning_rate\": \"adaptive\"}], [{\"eta0\": 1e-4}]]},\n",
    "{'name':'SVC', 'params': [[{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}],[{\"kernel\":\"linear\"}, {\"kernel\":\"rbf\"}, {\"kernel\":\"sigmoid\"}, {\"kernel\":\"poly\"}], [{\"degree\":1}, {\"degree\":2}, {\"degree\":3}, {\"degree\":4}]]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair all possible parameters of a type with others of a different types... basically combinations of n by k from the parameters\n",
    "import itertools\n",
    "\n",
    "# count models for stats\n",
    "total_nr_models = 0\n",
    "# store params list by model\n",
    "params_by_model = []\n",
    "\n",
    "for model_family in all_possible_model_params:\n",
    "    params = model_family[\"params\"]\n",
    "\n",
    "    # initialize an empty param dict for model\n",
    "    param_dict = []\n",
    "    # combinations of n by k from unique available params\n",
    "    combinations = [p for p in itertools.product(*params)]\n",
    "    # concatenate each tuple of unique params into a dict and save into param_dict\n",
    "    for combination in combinations:\n",
    "        param_dict.append(dict(itertools.chain.from_iterable(d.items() for d in combination)))\n",
    "\n",
    "    # count models\n",
    "    total_nr_models = total_nr_models + len(param_dict)\n",
    "\n",
    "    # append param list asssociated with model name\n",
    "    params_by_model.append({\"name\": model_family[\"name\"], \"params\": param_dict})\n",
    "    \n",
    "    # debug\n",
    "    # print(param_dict)\n",
    "    # break\n",
    "\n",
    "print(total_nr_models) # f*ck that's a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(params_by_model))\n",
    "model_classes = []\n",
    "i = 0\n",
    "\n",
    "for clf_name, clf in estimators_trimmed:\n",
    "    \n",
    "    # some errors without, unknown casting bug ?\n",
    "    clf_name = str(clf_name)\n",
    "\n",
    "    # make sure we are not passing the wrong parameters to the classifier\n",
    "    if params_by_model[i][\"name\"] == clf_name:\n",
    "        clf_params = params_by_model[i][\"params\"]\n",
    "    else:\n",
    "        print(\"Something went wrong! - Tried passing params from \" + params_by_model[i][\"name\"] + \" to \" + clf_name + \"!\\n\")\n",
    "\n",
    "    model_classes.append([clf, clf_name, clf_params])\n",
    "    # debug\n",
    "    # print(\"Appended \" + str(clf_name) + \" with params \" + str(clf_params) + \"to object \" + str(clf))\n",
    "    # break\n",
    "    \n",
    "    # increment our index for the parameter lookup\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# let's get some statistics about this process\n",
    "global_stats = {\n",
    "    \"trained_models\": 0,\n",
    "    \"failed_models\": 0,\n",
    "    \"total_models\": total_nr_models,\n",
    "    \"training_time\": 0,\n",
    "    \"total_training_time\": 0\n",
    "}\n",
    "\n",
    "model_stats = []\n",
    "\n",
    "# let's also time the execution of these, and total execution :)\n",
    "import time\n",
    "# total time start\n",
    "t_start = time.time()\n",
    "# var for longest model train time\n",
    "longest_total_model_train_time =0\n",
    "\n",
    "insights = []\n",
    "for Model, modelname, params_list in model_classes:\n",
    "\n",
    "    # check that params are initialized\n",
    "    if params_list != []:\n",
    "\n",
    "        # model timer start\n",
    "        t_start_model = time.time()\n",
    "        # longest param train time\n",
    "        longest_param_train_time = 0\n",
    "\n",
    "        for params in params_list:\n",
    "            # param timer start\n",
    "            t_start_param = time.time()\n",
    "\n",
    "            print(\"Starting \" + str(modelname) + \" with params: \" + str(params) + \"... \" + str(global_stats[\"trained_models\"] + global_stats[\"failed_models\"]) + \" OUT OF \" + str(global_stats[\"total_models\"]))\n",
    "\n",
    "            # define insight vars in case of failure\n",
    "            model_accuracy = 0\n",
    "            model_log_loss = 0\n",
    "            model_precision = 0\n",
    "            model_recall = 0\n",
    "            model_f1 = 0\n",
    "\n",
    "            # in case something goes wrong or unsupported params\n",
    "            try:\n",
    "                # some models fail when called with args ? unknown bug\n",
    "                model = Model\n",
    "                # some models fail with *kwargs\n",
    "                model.set_params(**params)\n",
    "                model.fit(df_training_categorical, JSat_training)\n",
    "\n",
    "                JSat_predicted = model.predict(df_validation_categorical)\n",
    "                \n",
    "                # compute model accuracy\n",
    "                model_accuracy = metrics.accuracy_score(JSat_validation, JSat_predicted)\n",
    "                # compute model log loss\n",
    "                # model_log_loss = metrics.log_loss(JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                # compute model precision\n",
    "                model_precision = metrics.precision_score(JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                #  compute model precision\n",
    "                model_recall = metrics.recall_score(JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                # compute model f1 score\n",
    "                model_f1 = 2*(model_precision*model_recall)/(model_precision + model_recall)\n",
    "\n",
    "                # param timer end\n",
    "                t_end_param = time.time()\n",
    "                # compute elapsed\n",
    "                param_time = t_end_param - t_start_param\n",
    "\n",
    "                print(\" finished in \" + str(param_time) + \"s; had an f1 of: \" + str(model_f1) + \"\\n\")\n",
    "                global_stats[\"trained_models\"] += 1\n",
    "            except Exception as e:\n",
    "                print(\" failed in \" + str(param_time) + \"s;\\n\")\n",
    "                print(e)\n",
    "                global_stats[\"failed_models\"] += 1\n",
    "\n",
    "            if longest_param_train_time < param_time:\n",
    "                longest_param_train_time = param_time\n",
    "            \n",
    "            insights.append((modelname, model, params, model_accuracy, model_f1, model_log_loss, model_precision, model_recall, param_time))\n",
    "        \n",
    "        # model timer end\n",
    "        t_end_model = time.time()\n",
    "        # get total model train time\n",
    "        model_time = t_end_model - t_start_model\n",
    "        # compare longest total model train time\n",
    "        if longest_total_model_train_time < model_time:\n",
    "            longest_total_model_train_time = model_time\n",
    "\n",
    "        # let's get stats by model\n",
    "        model_stats.append({\n",
    "            \"name\": modelname, \n",
    "            \"tuned_params\": len(params),\n",
    "            \"variations_tested\": len(params_list), \n",
    "            \"total_train_time\": t_end_model - t_start_model, \n",
    "            \"longest_train_time\": longest_param_train_time,\n",
    "            })\n",
    "\n",
    "# total time end\n",
    "t_end = time.time()\n",
    "# compute total time\n",
    "t_total = t_end - t_start\n",
    "\n",
    "global_stats[\"total_training_time\"] = t_total\n",
    "\n",
    "print(\"\\nFinalized all trainings in \" + str(t_total) + \"s. Phew!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize plots by different model parameters\n",
    "plt_models_by_clf = {}\n",
    "\n",
    "# reformat insights data to easier fit our plot requirements\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    # check if key exists and if not add new dict key with model type (name)\n",
    "    if not model_name in plt_models_by_clf:\n",
    "        plt_models_by_clf.update((model_name, []))\n",
    "    \n",
    "    plt_models_by_clf[model_name].append({\n",
    "        \"model_class\": model_class,\n",
    "        \"model_params\": model_params,\n",
    "        \"model_accuracy\": model_accuracy,\n",
    "        \"model_f1\": model_f1,\n",
    "        \"model_fbeta\": model_fbeta,\n",
    "        \"model_log_loss\": model_log_loss,\n",
    "        \"model_precision\": model_precision,\n",
    "        \"model_time\": model_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best accuracy models and compare them to the rest\n",
    "plt_model_best = []\n",
    "_count = 0\n",
    "for model_name, model_class, model_params, model_score in insights:\n",
    "    if not model_name in plt_model_best:\n",
    "        plt_model_best.append({\"name\": model_name, \"parameters\": model_params, \"accuracy\": model_score})\n",
    "    else:\n",
    "        for best_name, best_parameters, best_accuracy in plt_model_best:\n",
    "            _count = _count + 1\n",
    "            if best_name == model_name:\n",
    "                if best_accuracy < model_score:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"accuracy\"] = model_score\n",
    "                if best_accuracy == model_score:\n",
    "                    plt_model_best[_count][\"parameters\"] = best_parameters + model_params\n",
    "\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6a3af7cc3dd87b1c9aec9c941132942751acefef11dd989f38e57e7ca82ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
