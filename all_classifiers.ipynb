{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "estimators = all_estimators(type_filter='classifier')\n",
    "\n",
    "def getClassifiers():\n",
    "    all_clfs_names = []\n",
    "    all_clfs_clf = []\n",
    "    for name, ClassifierClass in estimators:\n",
    "        try:\n",
    "            clf = ClassifierClass()\n",
    "            all_clfs_names.append(name)\n",
    "            all_clfs_clf.append(clf)\n",
    "\n",
    "            # develop\n",
    "            # print(\"{'name':'\" + name + \"', 'params': [[{}],[{}]]},\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            # print('Unable to import', name)\n",
    "            # print(e)\n",
    "    return all_clfs_names, all_clfs_clf\n",
    "\n",
    "df = pd.read_csv(r'data/data_processed.csv')\n",
    "df_continous = pd.read_csv(r'data/data_continous.csv')\n",
    "df_categorical = pd.read_csv(r'data/data_categorical.csv')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# copying job satisfaction into new array\n",
    "JSat = df['JobSatisfaction'].values\n",
    "\n",
    "# trying only binary classification\n",
    "# any value of satisfaction greater than two will be \"Satisfied\" and anything less or equal will be \"Unsatisfied\"\n",
    "JSat_threshold = 2\n",
    "\n",
    "JSat_binary = [0 if val <= JSat_threshold else val for val in JSat]\n",
    "JSat_binary = [1 if val > JSat_threshold else val for val in JSat_binary]\n",
    "print(JSat_binary)\n",
    "\n",
    "# splitting inputs by row index\n",
    "# continous data\n",
    "df_training_continous = df_continous.iloc[:1200,:]\n",
    "df_validation_continous = df_continous.iloc[1200:,:]\n",
    "# categorical data\n",
    "df_training_categorical = df_categorical.iloc[:1200,:]\n",
    "df_validation_categorical = df_categorical.iloc[1200:,:]\n",
    "# splitting outputs by number\n",
    "JSat_training = JSat[:1200]\n",
    "JSat_validation = JSat[1200:]\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "\n",
    "all_clfs_names, all_clfs_clf = getClassifiers()\n",
    "estimators_trimmed = zip(all_clfs_names, all_clfs_clf)\n",
    "\n",
    "# remove RidgeClassifierCV because it's included in RidgeClassifier with this script\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"RidgeClassifierCV\"]\n",
    "# remove because it should only be used with ensemble methods and is out of scope rn\n",
    "estimators_trimmed = [(i,j) for i,j in estimators_trimmed if i != \"ExtraTreeClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define possible parameters for all models, this is the mother of config files :)\n",
    "all_possible_model_params = [\n",
    "{'name':'AdaBoostClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"random_state\":None}],[{\"learning_rate\":0.1}, {\"learning_rate\":1}, {\"learning_rate\":10}, {\"learning_rate\":100}]]}, # can implement estimator = any decisiion tree classifier with params... default = DecisionTreeClassifier(max_depth=1)\n",
    "{'name':'BaggingClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}, {\"n_estimators\":10}, {\"n_estimators\":5}],[{\"max_samples\": 1.0},{\"max_samples\": 2.0},{\"max_samples\": 0.1},{\"max_samples\": 0.5},{\"max_samples\": 1},{\"max_samples\": 10}],[{\"max_features\": 1.0},{\"max_features\": 2.0},{\"max_features\": 2.5},{\"max_features\": 10}],[{\"bootstrap\": False}, {\"bootstrap\": True}]]}, # same as above\n",
    "{'name':'BernoulliNB', 'params': [[{\"alpha\": 1.0}, {\"alpha\": 100.0}, {\"alpha\": 1e-12}, {\"alpha\": 1e-6}, {\"alpha\": 1e-3}], [{\"binarize\": None}, {\"binarize\": 0.0}, {\"binarize\": 1.0}, {\"binarize\": 2.5}, {\"binarize\": 5.0}], [{\"fit_prior\": True}, {\"fit_prior\": False}]]},\n",
    "{'name':'CalibratedClassifierCV', 'params': [[{'method':'sigmoid'}, {'method:':'isotonic'}] ,[{\"cv\": None}, {\"cv\": 1}, {\"cv\": 3}, {\"cv\": 10}]]},\n",
    "{'name':'CategoricalNB', 'params': [[{\"fit_prior\": True}, {\"fit_prior\": False}],[{\"alpha\": 0}, {\"alpha\": 1}, {\"alpha\": 3}, {\"alpha\": 5}]]},\n",
    "{'name':'ComplementNB', 'params': [[{\"fit_prior\": True}, {\"fit_prior\": False}],[{\"norm\": True}, {\"norm\": False}], [{\"alpha\": 0}, {\"alpha\": 1}, {\"alpha\": 3}, {\"alpha\": 5}]]},\n",
    "{'name':'DecisionTreeClassifier', 'params': [[{\"splitter\": \"best\"}, {\"splitter\": \"random\"}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 1}, {\"min_samples_split\": 2}]]},\n",
    "{'name':'DummyClassifier', 'params': [[{\"strategy\": \"prior\"}],[{\"random_state\": None}]]},\n",
    "{'name':'ExtraTreesClassifier', 'params': [[{\"n_estimators\":100}, {\"n_estimators\":50}, {\"n_estimators\":200}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 10}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}, {\"max_features\": None}], [{\"min_samples_leaf\": 1}, {\"min_samples_leaf\": 3}, {\"min_samples_leaf\": 5}, {\"min_samples_leaf\": 10}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 3}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": \"balanced_subsample\"}]]},\n",
    "{'name':'GaussianNB', 'params': [[{\"var_smoothing\": 1e-9}, {\"var_smoothing\": 1e-8}, {\"var_smoothing\": 1e-10}],[{\"priors\": None}]]},\n",
    "{'name':'GaussianProcessClassifier', 'params': [[{\"max_iter_predict\": 100}, {\"max_iter_predict\": 400}, {\"max_iter_predict\": 1000}],[{\"n_jobs\": -1}], [{\"multi_class\": \"one_vs_rest\"}, {\"multi_class\": \"one_vs_one\"}]]},\n",
    "{'name':'GradientBoostingClassifier', 'params': [[{\"loss\": \"log_loss\"}, {\"loss\": \"exponential\"}],[{\"n_estimators\": 100}, {\"n_estimators\": 10}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e5}, {\"n_estimators\": 1e10}, {\"n_estimators\": 5}], [{\"subsample\": 1}, {\"subsample\": 0.9}, {\"subsample\": 0.5}, {\"subsample\": 0.3}],[{\"min_samples_split\": 2}, {\"min_samples_split\": 4}, {\"min_samples_split\": 8}], [{\"max_depth\": None}, {\"max_depth\": 3}], [{\"max_features\": \"auto\"}, {\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}, {\"max_features\": None}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 1}, {\"ccp_alpha\": 10}, {\"ccp_alpha\": 10}]]},\n",
    "{'name':'HistGradientBoostingClassifier', 'params': [[{\"max_iter\": 100}, {\"max_iter\": 1000}, {\"max_iter\": 500}],[{\"max_leaf_nodes\": 31}, {\"max_leaf_nodes\": None}], [{\"min_samples_leaf\": 20}, {\"min_samples_leaf\": 10}], [{\"max_bins\": 255}, {\"max_bins\": 200}, {\"max_bins\": 150}, {\"max_bins\": 75}]]},\n",
    "{'name':'KNeighborsClassifier', 'params': [[{\"n_neighbors\": 5}],[{\"weights\": \"uniform\"}, {\"weights\": \"distance\"}], [{\"leaf_size\": 10}, {\"leaf_size\": 20}, {\"leaf_size\": 30}, {\"leaf_size\": 40}, {\"leaf_size\": 5}, {\"leaf_size\": 15}, {\"leaf_size\": 25}, {\"leaf_size\": 35}, {\"leaf_size\": 50}], [{\"p\": 2}, {\"p\": 1}, {\"p\": 3}, {\"p\": 4}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'LabelPropagation', 'params': [[{\"kernel\": \"rnn\"}, {\"kernel\": \"rbf\"}],[{\"gamma\": 20}, {\"gamma\": 10}, {\"gamma\": 5}, {\"gamma\": 40}], [{\"n_neighbors\": 1}, {\"n_neighbors\": 3}, {\"n_neighbors\": 7}, {\"n_neighbors\": 10}, {\"n_neighbors\": 20}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}], [{\"n_jobs\": -1}], [{\"tol\": 1e-3}, {\"tol\": 1e-2}, {\"tol\": 1e-4}]]},\n",
    "{'name':'LabelSpreading', 'params': [[{\"kernel\": \"rnn\"}, {\"kernel\": \"rbf\"}],[{\"gamma\": 20}, {\"gamma\": 10}, {\"gamma\": 5}, {\"gamma\": 40}], [{\"n_neighbors\": 1}, {\"n_neighbors\": 3}, {\"n_neighbors\": 7}, {\"n_neighbors\": 10}, {\"n_neighbors\": 20}], [{\"max_iter\": 300}, {\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 30}], [{\"n_jobs\": -1}], [{\"alpha\": 0.2}, {\"alpha\": 0.1}, {\"alpha\": 0}, {\"alpha\": 0.5}, {\"alpha\": 0.9}, {\"alpha\": 1}]]},\n",
    "{'name':'LinearDiscriminantAnalysis', 'params': [[{\"solver\": \"svd\"}, {\"solver\": \"lsqr\"}, {\"solver\": \"eigen\"}],[{\"shrinkage\": None}, {\"shrinkage\": \"auto\"}]]},\n",
    "{'name':'LinearSVC', 'params': [[{\"C\": 1}, {\"C\": 0.5}, {\"C\": 0.9}, {\"C\": 2}],[{\"multi_class\": \"ovr\"}, {\"multi_class\": \"crammer_singer\"}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}]]},\n",
    "{'name':'LogisticRegression', 'params': [[{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}],[{\"solver\":'lbfgs'}, {\"solver\":'liblinear'}, {\"solver\":'newton-cg'}, {\"solver\":'sag'}, {\"solver\":'saga'}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"n_jobs\": -1}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}, {\"max_iter\": 4000}]]},\n",
    "{'name':'LogisticRegressionCV', 'params': [[{\"cv\": None}, {\"cv\": 1}, {\"cv\": 3}, {\"cv\": 5}, {\"cv\": 8}, {\"cv\": 1}],[{\"Cs\":0.01}, {\"Cs\":0.1}, {\"Cs\":1}, {\"Cs\":10}],[{\"solver\":'lbfgs'}, {\"solver\":'liblinear'}, {\"solver\":'newton-cg'}, {\"solver\":'sag'}, {\"solver\":'saga'}], [{\"intercept_scaling\": 1}, {\"intercept_scaling\": 2}, {\"intercept_scaling\": 3}, {\"intercept_scaling\": 5}], [{\"class_weight\": \"balanced\"}, {\"class_weight\": None}], [{\"n_jobs\": -1}], [{\"max_iter\": 1000}, {\"max_iter\": 2000}]]},\n",
    "{'name':'MLPClassifier', 'params': [[{\"hidden_layer_sizes\": 100}, {\"hidden_layer_sizes\": 35}, {\"hidden_layer_sizes\": 200}, {\"hidden_layer_sizes\": 500}],[{\"activation\": \"identity\"}, {\"activation\": \"logistic\"}, {\"activation\": \"tanh\"}, {\"activation\": \"relu\"}],[{\"solver\": \"lbfgs\"}, {\"solver\": \"sgd\"}, {\"solver\": \"adam\"}], [{\"alpha\": 1e-4}, {\"alpha\": 1e-5}, {\"alpha\": 1e-3}, {\"alpha\": 1e-6}], [{\"learning_rate\": \"constant\"}, {\"learning_rate\": \"invscaling\"}, {\"learning_rate\": \"adaptive\"}], [{\"max_iter\": 200}, {\"max_iter\": 2000}]]},\n",
    "{'name':'MultinomialNB', 'params': [[{\"alpha\": 0}, {\"alpha\": 1.0}, {\"alpha\": 100.0}, {\"alpha\": 1e-12}, {\"alpha\": 1e-6}, {\"alpha\": 1e-3}],[{\"fit_prior\": True}, {\"fit_prior\": False}]]},\n",
    "{'name':'NearestCentroid', 'params': [[{\"metric\": \"euclidean\"}, {\"metric\": \"manhattan\"}, {\"metric\": \"cosine\"}, {\"metric\": \"haversine\"}, {\"metric\": \"cityblock\"}],[{\"shrink_threshold\": None}, {\"shrink_threshold\": 1e-4}, {\"shrink_threshold\": 1e4}]]},\n",
    "{'name':'NuSVC', 'params': [[{\"C\":1e-4}, {\"C\":1e-3}, {\"C\":1e-2}, {\"C\":0.1}, {\"C\":1}],[{\"kernel\":\"linear\"}, {\"kernel\":\"rbf\"}, {\"kernel\":\"sigmoid\"}, {\"kernel\":\"poly\"}], [{\"degree\":1}, {\"degree\":2}, {\"degree\":3}, {\"degree\":4}]]},\n",
    "{'name':'PassiveAggressiveClassifier', 'params': [[{\"C\":1e-4}, {\"C\":1e-3}, {\"C\":1e-2}, {\"C\":0.1}, {\"C\":1}],[{\"max_iter\": 1000}, {\"max_iter\": -1}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'Perceptron', 'params': [[{\"penalty\": \"l1\"}, {\"penalty\": \"l2\"}, {\"penalty\": \"elasticnet\"}, {\"penalty\": None}],[{\"alpha\": 1e-4}, {\"alpha\": 1e-3}, {\"alpha\": 1e-2}, {\"alpha\": 1e-5}], [{\"l1_ratio\": 0.15}, {\"l1_ratio\": 0.05}, {\"l1_ratio\": 0.25}, {\"l1_ratio\": 0.5}, {\"l1_ratio\": 0.75}]]},\n",
    "{'name':'QuadraticDiscriminantAnalysis', 'params': [[{\"reg_param\": 0.1}, {\"reg_param\": 0.2}, {\"reg_param\": 0.3}, {\"reg_param\": 0.4}, {\"reg_param\": 0.5}],[{\"tol\": 1e-4}, {\"tol\": 1e-3}, {\"tol\": 1e-6}]]},\n",
    "{'name':'RadiusNeighborsClassifier', 'params': [[{\"radius\": 1}, {\"radius\": 2}, {\"radius\": 3}, {\"radius\": 1e-1}, {\"radius\": 1e-2}],[{\"weights\": \"uniform\"}, {\"weights\": \"distance\"}], [{\"algorithm\": \"auto\"}, {\"algorithm\": \"ball_tree\"}, {\"algorithm\": \"kd_tree\"}, {\"algorithm\": \"brute\"}], [{\"p\": 1}, {\"p\": 2}], [{\"metric\": \"euclidean\"}, {\"metric\": \"manhattan\"}, {\"metric\": \"cosine\"}, {\"metric\": \"haversine\"}, {\"metric\": \"cityblock\"}, {\"metric\": \"minkowski\"}], [{\"n_jobs\": -1}]]},\n",
    "{'name':'RandomForestClassifier', 'params': [[{\"n_estimators\": 100}, {\"n_estimators\": 1e4}, {\"n_estimators\": 1e6}, {\"n_estimators\": 10}],[{\"criterion\": \"gini\"}, {\"criterion\": \"entropy\"}, {\"criterion\": \"log_loss\"}], [{\"min_samples_split\": 2}, {\"min_samples_split\": 3}, {\"min_samples_split\": 5}, {\"min_samples_split\": 1}], [{\"max_features\": \"sqrt\"}, {\"max_features\": \"log2\"}], [{\"bootstrap\": True}, {\"bootstrap\": False}], [{\"n_jobs\": -1}, {\"n_jobs\": None}], [{\"ccp_alpha\": 0}, {\"ccp_alpha\": 0.1}, {\"ccp_alpha\": 0.01}]]},\n",
    "{'name':'RidgeClassifier', 'params': [[{\"solver\": \"svd\"}, {\"solver\": \"lsqr\"}, {\"solver\": \"cholesky\"}, {\"solver\": \"sparse_cg\"}, {\"solver\": \"sag\"}, {\"solver\": \"saga\"}, {\"solver\": \"lbfgs\"}], [{\"alpha\": 1}, {\"alpha\": 1e-2}, {\"alpha\": 1e-4}, {\"alpha\": 1e2}, {\"alpha\": 1e4}],[{\"positive\": True}, {\"positive\": False}]]},\n",
    "{'name':'SGDClassifier', 'params': [[{\"loss\": \"hinge\"}, {\"loss\": \"log_loss\"}, {\"loss\": \"modified_huber\"}, {\"loss\": \"squared_hinge\"}, {\"loss\": \"perceptron\"}, {\"loss\": \"squared_error\"}, {\"loss\": \"huber\"}, {\"loss\": \"epsilon_insensitive\"}, {\"loss\": \"squared_epsilon_insensitive\"}],[{\"penalty\": \"l1\"}, {\"penalty\": \"l2\"}, {\"penalty\": \"elasticnet\"}, {\"penalty\": None}], [{\"alpha\": 1e-4}, {\"alpha\": 1e-3}, {\"alpha\": 1e-2}, {\"alpha\": 1e-1}, {\"alpha\": 1}, {\"alpha\": 1e2}, {\"alpha\": 1e3}, {\"alpha\": 1e4}], [{\"l1_ratio\": 0.15}, {\"l1_ratio\": 0.05}, {\"l1_ratio\": 0.25}, {\"l1_ratio\": 0.5}, {\"l1_ratio\": 0.75}], [{\"max_iter\": 1e2}, {\"max_iter\": 1e3}], [{\"n_jobs\": None}, {\"n_jobs\": -1}], [{\"epsilon\": 1e-1}, {\"epsilon\": 1}, {\"epsilon\": 1e-2}], [{\"learning_rate\": \"constant\"}, {\"learning_rate\": \"optimal\"}, {\"learning_rate\": \"invscaling\"}, {\"learning_rate\": \"adaptive\"}], [{\"eta0\": 1e-4}]]},\n",
    "{'name':'SVC', 'params': [[{\"C\":0.01}, {\"C\":0.1}, {\"C\":1}, {\"C\":10}],[{\"kernel\":\"linear\"}, {\"kernel\":\"rbf\"}, {\"kernel\":\"sigmoid\"}, {\"kernel\":\"poly\"}], [{\"degree\":1}, {\"degree\":2}, {\"degree\":3}, {\"degree\":4}]]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86252\n"
     ]
    }
   ],
   "source": [
    "# pair all possible parameters of a type with others of a different types... basically combinations of n by k from the parameters\n",
    "import itertools\n",
    "\n",
    "# count models for stats\n",
    "total_nr_models = 0\n",
    "# store params list by model\n",
    "params_by_model = []\n",
    "\n",
    "for model_family in all_possible_model_params:\n",
    "    params = model_family[\"params\"]\n",
    "\n",
    "    # initialize an empty param dict for model\n",
    "    param_dict = []\n",
    "    # combinations of n by k from unique available params\n",
    "    combinations = [p for p in itertools.product(*params)]\n",
    "    # concatenate each tuple of unique params into a dict and save into param_dict\n",
    "    for combination in combinations:\n",
    "        param_dict.append(dict(itertools.chain.from_iterable(d.items() for d in combination)))\n",
    "\n",
    "    # count models\n",
    "    total_nr_models = total_nr_models + len(param_dict)\n",
    "\n",
    "    # append param list asssociated with model name\n",
    "    params_by_model.append({\"name\": model_family[\"name\"], \"params\": param_dict})\n",
    "    \n",
    "    # debug\n",
    "    # print(param_dict)\n",
    "    # break\n",
    "\n",
    "print(total_nr_models) # f*ck that's a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(params_by_model))\n",
    "model_classes = []\n",
    "i = 0\n",
    "\n",
    "for clf_name, clf in estimators_trimmed:\n",
    "    \n",
    "    # some errors without, unknown casting bug ?\n",
    "    clf_name = str(clf_name)\n",
    "\n",
    "    # make sure we are not passing the wrong parameters to the classifier\n",
    "    if params_by_model[i][\"name\"] == clf_name:\n",
    "        clf_params = params_by_model[i][\"params\"]\n",
    "    else:\n",
    "        print(\"Something went wrong! - Tried passing params from \" + params_by_model[i][\"name\"] + \" to \" + clf_name + \"!\\n\")\n",
    "\n",
    "    model_classes.append([clf, clf_name, clf_params])\n",
    "    # debug\n",
    "    # print(\"Appended \" + str(clf_name) + \" with params \" + str(clf_params) + \"to object \" + str(clf))\n",
    "    # break\n",
    "    \n",
    "    # increment our index for the parameter lookup\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVC with params: {'C': 10, 'kernel': 'poly', 'degree': 4}... 86251 OUT OF 86252\n",
      " finished in 0.46645236015319824s; had an f1 of: 0.4444444444444444\n",
      "\n",
      "\n",
      "Finalized all trainings in 14533.979711055756s. Phew!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import metrics\n",
    "\n",
    "# so we can hopefully clear some output cell from overflowing...\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# let's get some statistics about this process\n",
    "global_stats = {\n",
    "    \"trained_models\": 0,\n",
    "    \"failed_models\": 0,\n",
    "    \"total_models\": total_nr_models,\n",
    "    \"training_time\": 0,\n",
    "    \"total_training_time\": 0\n",
    "}\n",
    "\n",
    "model_stats = []\n",
    "\n",
    "# let's also time the execution of these, and total execution :)\n",
    "# total time start\n",
    "t_start = time.time()\n",
    "# var for longest model train time\n",
    "longest_total_model_train_time = 0\n",
    "\n",
    "insights = []\n",
    "for Model, modelname, params_list in model_classes:\n",
    "\n",
    "    # check that params are initialized\n",
    "    if params_list != []:\n",
    "\n",
    "        # model timer start\n",
    "        t_start_model = time.time()\n",
    "        # longest param train time\n",
    "        longest_param_train_time = 0\n",
    "\n",
    "        for params in params_list:\n",
    "            # param timer start\n",
    "            t_start_param = time.time()\n",
    "\n",
    "            clear_output()\n",
    "\n",
    "            print(\"Starting \" + str(modelname) + \" with params: \" + str(params) + \"... \" +\n",
    "                  str(global_stats[\"trained_models\"] + global_stats[\"failed_models\"]) + \" OUT OF \" + str(global_stats[\"total_models\"]))\n",
    "\n",
    "            # define insight vars in case of failure\n",
    "            model_accuracy = 0\n",
    "            model_log_loss = 0\n",
    "            model_precision = 0\n",
    "            model_recall = 0\n",
    "            model_f1 = 0\n",
    "\n",
    "            # in case something goes wrong or unsupported params\n",
    "            try:\n",
    "                # some models fail when called with args ? unknown bug\n",
    "                model = Model\n",
    "                # some models fail with *kwargs\n",
    "                model.set_params(**params)\n",
    "                model.fit(df_training_categorical, JSat_training)\n",
    "\n",
    "                JSat_predicted = model.predict(df_validation_categorical)\n",
    "\n",
    "                # compute model accuracy\n",
    "                model_accuracy = metrics.accuracy_score(\n",
    "                    JSat_validation, JSat_predicted)\n",
    "                # compute model log loss\n",
    "                # model_log_loss = metrics.log_loss(JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                # compute model precision\n",
    "                model_precision = metrics.precision_score(\n",
    "                    JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                #  compute model precision\n",
    "                model_recall = metrics.recall_score(\n",
    "                    JSat_validation, JSat_predicted, average=\"micro\")\n",
    "                # compute model f1 score\n",
    "                model_f1 = 2*(model_precision*model_recall) / \\\n",
    "                    (model_precision + model_recall)\n",
    "\n",
    "                # param timer end\n",
    "                t_end_param = time.time()\n",
    "                # compute elapsed\n",
    "                param_time = t_end_param - t_start_param\n",
    "\n",
    "                print(\" finished in \" + str(param_time) +\n",
    "                      \"s; had an f1 of: \" + str(model_f1) + \"\\n\")\n",
    "                global_stats[\"trained_models\"] += 1\n",
    "            except Exception as e:\n",
    "                print(\" failed in \" + str(param_time) + \"s;\\n\")\n",
    "                print(e)\n",
    "                global_stats[\"failed_models\"] += 1\n",
    "\n",
    "            if longest_param_train_time < param_time:\n",
    "                longest_param_train_time = param_time\n",
    "\n",
    "            insights.append((modelname, model, params, model_accuracy, model_f1,\n",
    "                            model_log_loss, model_precision, model_recall, param_time))\n",
    "\n",
    "        # model timer end\n",
    "        t_end_model = time.time()\n",
    "        # get total model train time\n",
    "        model_time = t_end_model - t_start_model\n",
    "        # compare longest total model train time\n",
    "        if longest_total_model_train_time < model_time:\n",
    "            longest_total_model_train_time = model_time\n",
    "\n",
    "        # let's get stats by model\n",
    "        model_stats.append({\n",
    "            \"name\": modelname,\n",
    "            \"tuned_params\": len(params),\n",
    "            \"variations_tested\": len(params_list),\n",
    "            \"total_train_time\": t_end_model - t_start_model,\n",
    "            \"longest_train_time\": longest_param_train_time,\n",
    "        })\n",
    "\n",
    "# total time end\n",
    "t_end = time.time()\n",
    "# compute total time\n",
    "t_total = t_end - t_start\n",
    "\n",
    "global_stats[\"total_training_time\"] = t_total\n",
    "\n",
    "print(\"\\nFinalized all trainings in \" + str(t_total) + \"s. Phew!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 23-01-2023-16-13-27\n"
     ]
    }
   ],
   "source": [
    "# in case I forgot something (and damn it I did... so many times...) and because the computation time is long I don't want to waste all the computation that I already did, so let's save it into a log file\n",
    "# maybe I should've spent more time on a way to cache the models... not sure I have enough space for that. Hey! Guess I should've used google collab...\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# datetime object containing current date and time\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "print(\"date and time =\", dt_string)\n",
    "\n",
    "# save current insights into a file\n",
    "with open(\"insights-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(insights, f)\n",
    "with open(\"globalstats-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(global_stats, f)\n",
    "with open(\"modelstats-\" + dt_string + \".pck\", 'wb') as f:\n",
    "    pickle.dump(model_stats, f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.23 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "# ^^^^ prevents this cell from being executed automatically\n",
    "# load an insights log file\n",
    "load_file_name = \"insights-23-01-2023-16-13-27\" + \".pck\"\n",
    "with open(load_file_name, 'rb') as f:\n",
    "    insights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize plots by different model parameters\n",
    "plt_models_by_clf = {}\n",
    "\n",
    "# reformat insights data to easier fit our plot requirements\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    # check if key exists and if not add new dict key with model type (name)\n",
    "    if not model_name in plt_models_by_clf:\n",
    "        plt_models_by_clf.update({str(model_name): []})\n",
    "    \n",
    "    plt_models_by_clf[model_name].append({\n",
    "        \"model_class\": model_class,\n",
    "        \"model_params\": model_params,\n",
    "        \"model_accuracy\": model_accuracy,\n",
    "        \"model_f1\": model_f1,\n",
    "        \"model_fbeta\": model_fbeta,\n",
    "        \"model_log_loss\": model_log_loss,\n",
    "        \"model_precision\": model_precision,\n",
    "        \"model_time\": model_time\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick best accuracy models and compare them to the rest\n",
    "plt_model_best = []\n",
    "_count = 0\n",
    "for model_name, model_class, model_params, model_accuracy, model_f1, model_fbeta, model_log_loss, model_precision, model_time in insights:\n",
    "    mdl_found = False\n",
    "    for mdl in plt_model_best:\n",
    "        if mdl[\"name\"] == model_name:\n",
    "            mdl_found = True\n",
    "    \n",
    "    if not mdl_found:\n",
    "        plt_model_best.append({\"name\": model_name, \"parameters\": model_params, \"f1_score\": model_f1, \"model_time\": model_time})\n",
    "    else:\n",
    "        for best_name, best_parameters, best_f1, best_time in plt_model_best:\n",
    "            _count = _count + 1\n",
    "            if best_name == model_name:\n",
    "                print(best_name + \" == \" + model_name)\n",
    "                if best_f1 < model_f1:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"f1_score\"] = model_f1\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                if best_f1 == model_f1 and best_time == model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = best_parameters + model_params\n",
    "                if best_f1 == model_f1 and best_time > model_time:\n",
    "                    plt_model_best[_count][\"parameters\"] = model_params\n",
    "                    plt_model_best[_count][\"model_time\"] = model_time\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AdaBoostClassifier with best f1 score 1.0 and it took 0.2s to train\n",
      "Model BaggingClassifier with best f1 score 1.0 and it took 0.32s to train\n",
      "Model BernoulliNB with best f1 score 0.2037037037037037 and it took 0.03s to train\n",
      "Model CalibratedClassifierCV with best f1 score 0.774074074074074 and it took 1.39s to train\n",
      "Model CategoricalNB with best f1 score 0 and it took 2.67s to train\n",
      "Model ComplementNB with best f1 score 0.2222222222222222 and it took 0.01s to train\n",
      "Model DecisionTreeClassifier with best f1 score 0 and it took 0.01s to train\n",
      "Model DummyClassifier with best f1 score 0.2851851851851852 and it took 0.0s to train\n",
      "Model ExtraTreesClassifier with best f1 score 1.0 and it took 0.27s to train\n",
      "Model GaussianNB with best f1 score 1.0 and it took 0.01s to train\n",
      "Model GaussianProcessClassifier with best f1 score 0.2851851851851852 and it took 4.54s to train\n",
      "Model GradientBoostingClassifier with best f1 score 1.0 and it took 0.64s to train\n",
      "Model HistGradientBoostingClassifier with best f1 score 1.0 and it took 0.54s to train\n",
      "Model KNeighborsClassifier with best f1 score 0.2037037037037037 and it took 0.04s to train\n",
      "Model LabelPropagation with best f1 score 0 and it took 0.06s to train\n",
      "Model LabelSpreading with best f1 score 0 and it took 0.12s to train\n",
      "Model LinearDiscriminantAnalysis with best f1 score 0.26296296296296295 and it took 0.02s to train\n",
      "Model LinearSVC with best f1 score 0.35185185185185186 and it took 0.26s to train\n",
      "Model LogisticRegression with best f1 score 0.825925925925926 and it took 2.69s to train\n",
      "Model LogisticRegressionCV with best f1 score 0 and it took 2.42s to train\n",
      "Model MLPClassifier with best f1 score 0.25925925925925924 and it took 0.41s to train\n",
      "Model MultinomialNB with best f1 score 0.2222222222222222 and it took 0.01s to train\n",
      "Model NearestCentroid with best f1 score 0.2222222222222222 and it took 0.01s to train\n",
      "Model NuSVC with best f1 score 0 and it took 0.02s to train\n",
      "Model PassiveAggressiveClassifier with best f1 score 0.2851851851851852 and it took 0.04s to train\n",
      "Model Perceptron with best f1 score 0.2037037037037037 and it took 0.03s to train\n",
      "Model QuadraticDiscriminantAnalysis with best f1 score 1.0 and it took 0.02s to train\n",
      "Model RadiusNeighborsClassifier with best f1 score 0 and it took 0.01s to train\n",
      "Model RandomForestClassifier with best f1 score 1.0 and it took 0.23s to train\n",
      "Model RidgeClassifier with best f1 score 0 and it took 0.04s to train\n",
      "Model SGDClassifier with best f1 score 0.28888888888888886 and it took 0.01s to train\n",
      "Model SVC with best f1 score 1.0 and it took 2.86s to train\n"
     ]
    }
   ],
   "source": [
    "# print all models\n",
    "for model in plt_model_best:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AdaBoostClassifier with best f1 score 1.0 and it took 0.2s to train\n",
      "Model BaggingClassifier with best f1 score 1.0 and it took 0.32s to train\n",
      "Model ExtraTreesClassifier with best f1 score 1.0 and it took 0.27s to train\n",
      "Model GaussianNB with best f1 score 1.0 and it took 0.01s to train\n",
      "Model GradientBoostingClassifier with best f1 score 1.0 and it took 0.64s to train\n",
      "Model HistGradientBoostingClassifier with best f1 score 1.0 and it took 0.54s to train\n",
      "Model QuadraticDiscriminantAnalysis with best f1 score 1.0 and it took 0.02s to train\n",
      "Model RandomForestClassifier with best f1 score 1.0 and it took 0.23s to train\n",
      "Model SVC with best f1 score 1.0 and it took 2.86s to train\n"
     ]
    }
   ],
   "source": [
    "# print models with best f1 score\n",
    "best_f1 = 0\n",
    "best_models = []\n",
    "\n",
    "for model in plt_model_best:\n",
    "    if model[\"f1_score\"] >= best_f1:\n",
    "        best_models.append(model)\n",
    "        best_f1 = model[\"f1_score\"]\n",
    "\n",
    "for model in best_models:\n",
    "    print(\"Model \" + model[\"name\"] + \" with best f1 score \" + str(model[\"f1_score\"]) + \" and it took \" + str(round(model[\"model_time\"], 2)) + \"s to train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'GaussianNB', 'parameters': {'var_smoothing': 1e-09, 'priors': None}, 'f1_score': 1.0, 'model_time': 0.005997657775878906}\n"
     ]
    }
   ],
   "source": [
    "# print model with best f1 and train time\n",
    "best_time = 1000\n",
    "best_model = None\n",
    "\n",
    "for model in best_models:\n",
    "    if model[\"model_time\"] < best_time:\n",
    "        best_model = model\n",
    "        best_time = model[\"model_time\"]\n",
    "\n",
    "print(str(best_model))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6a3af7cc3dd87b1c9aec9c941132942751acefef11dd989f38e57e7ca82ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
