{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all the global statistics from all sessions\n",
    "import os, glob, pickle\n",
    "path = 'out/logs/'\n",
    "global_stats = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, 'globalstats-*.pck')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'rb') as f: # open in readonly mode\n",
    "        global_stats.append(pickle.load(f))\n",
    "\n",
    "# let's also include the mishandled logs I saved in text files... because I didn't find pickle in time\n",
    "global_stats.append({'trained_models': 79312, 'failed_models': 6940, 'total_models': 86252, 'training_time': 0, 'total_training_time': 13077.338770151138})\n",
    "global_stats.append({'trained_models': 14261, 'failed_models': 8340, 'total_models': 22601, 'training_time': 0, 'total_training_time': 8077.299234761134}) # stopped early, actually had 124k models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get all the model statistics from all sessions\n",
    "import os, glob, pickle\n",
    "path = 'out/logs/'\n",
    "model_stats = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, 'modelstats-*.pck')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'rb') as f: # open in readonly mode\n",
    "        model_stats.append(pickle.load(f))\n",
    "\n",
    "# let's also include the mishandled logs I saved in text files... because I didn't find pickle in time\n",
    "model_stats.append([{'name': 'AdaBoostClassifier', 'tuned_params': 3, 'variations_tested': 12, 'total_train_time': 3.255725383758545, 'longest_train_time': 0.6214597225189209}, {'name': 'BaggingClassifier', 'tuned_params': 4, 'variations_tested': 240, 'total_train_time': 20.60045099258423, 'longest_train_time': 0.8947405815124512}, {'name': 'BernoulliNB', 'tuned_params': 3, 'variations_tested': 50, 'total_train_time': 0.5232503414154053, 'longest_train_time': 0.01401376724243164}, {'name': 'CalibratedClassifierCV', 'tuned_params': 2, 'variations_tested': 8, 'total_train_time': 5.729030609130859, 'longest_train_time': 3.458470582962036}, {'name': 'CategoricalNB', 'tuned_params': 2, 'variations_tested': 8, 'total_train_time': 0.09990429878234863, 'longest_train_time': 3.458470582962036}, {'name': 'ComplementNB', 'tuned_params': 3, 'variations_tested': 16, 'total_train_time': 0.1620030403137207, 'longest_train_time': 0.013008594512939453}, {'name': 'DecisionTreeClassifier', 'tuned_params': 3, 'variations_tested': 12, 'total_train_time': 0.10699605941772461, 'longest_train_time': 0.015001058578491211}, {'name': 'DummyClassifier', 'tuned_params': 2, 'variations_tested': 1, 'total_train_time': 0.00599980354309082, 'longest_train_time': 0.00599980354309082}, {'name': 'ExtraTreesClassifier', 'tuned_params': 9, 'variations_tested': 5184, 'total_train_time': 1223.0683274269104, 'longest_train_time': 0.6768836975097656}, {'name': 'GaussianNB', 'tuned_params': 2, 'variations_tested': 3, 'total_train_time': 0.02275872230529785, 'longest_train_time': 0.008690118789672852}, {'name': 'GaussianProcessClassifier', 'tuned_params': 3, 'variations_tested': 6, 'total_train_time': 9.931557416915894, 'longest_train_time': 4.39141058921814}, {'name': 'GradientBoostingClassifier', 'tuned_params': 7, 'variations_tested': 4608, 'total_train_time': 312.4716694355011, 'longest_train_time': 8.345917224884033}, {'name': 'HistGradientBoostingClassifier', 'tuned_params': 4, 'variations_tested': 48, 'total_train_time': 89.64710640907288, 'longest_train_time': 3.7873804569244385}, {'name': 'KNeighborsClassifier', 'tuned_params': 5, 'variations_tested': 72, 'total_train_time': 3.6369175910949707, 'longest_train_time': 0.12871813774108887}, {'name': 'LabelPropagation', 'tuned_params': 6, 'variations_tested': 480, 'total_train_time': 26.86554789543152, 'longest_train_time': 0.2099592685699463}, {'name': 'LabelSpreading', 'tuned_params': 6, 'variations_tested': 960, 'total_train_time': 67.04725742340088, 'longest_train_time': 0.3662123680114746}, {'name': 'LinearDiscriminantAnalysis', 'tuned_params': 2, 'variations_tested': 6, 'total_train_time': 0.06501150131225586, 'longest_train_time': 0.0189971923828125}, {'name': 'LinearSVC', 'tuned_params': 5, 'variations_tested': 256, 'total_train_time': 2346.9275286197662, 'longest_train_time': 29.740203857421875}, {'name': 'LogisticRegression', 'tuned_params': 6, 'variations_tested': 640, 'total_train_time': 609.0242576599121, 'longest_train_time': 3.815778970718384}, {'name': 'LogisticRegressionCV', 'tuned_params': 7, 'variations_tested': 3840, 'total_train_time': 6630.224061012268, 'longest_train_time': 28.333537578582764}, {'name': 'MLPClassifier', 'tuned_params': 6, 'variations_tested': 2304, 'total_train_time': 14743.986862182617, 'longest_train_time': 263.91276812553406}, {'name': 'MultinomialNB', 'tuned_params': 2, 'variations_tested': 12, 'total_train_time': 0.08374214172363281, 'longest_train_time': 0.008652687072753906}, {'name': 'NearestCentroid', 'tuned_params': 2, 'variations_tested': 15, 'total_train_time': 0.12792181968688965, 'longest_train_time': 0.01602482795715332}, {'name': 'NuSVC', 'tuned_params': 3, 'variations_tested': 80, 'total_train_time': 0.17728137969970703, 'longest_train_time': 0.006960391998291016}, {'name': 'PassiveAggressiveClassifier', 'tuned_params': 3, 'variations_tested': 10, 'total_train_time': 0.1239011287689209, 'longest_train_time': 0.025143146514892578}, {'name': 'Perceptron', 'tuned_params': 3, 'variations_tested': 80, 'total_train_time': 1.4140112400054932, 'longest_train_time': 0.062003374099731445}, {'name': 'QuadraticDiscriminantAnalysis', 'tuned_params': 2, 'variations_tested': 15, 'total_train_time': 0.18208527565002441, 'longest_train_time': 0.029015541076660156}, {'name': 'RadiusNeighborsClassifier', 'tuned_params': 6, 'variations_tested': 480, 'total_train_time': 9.393752336502075, 'longest_train_time': 0.052002906799316406}, {'name': 'RandomForestClassifier', 'tuned_params': 7, 'variations_tested': 1152, 'total_train_time': 69.12116956710815, 'longest_train_time': 0.4061722755432129}, {'name': 'RidgeClassifier', 'tuned_params': 3, 'variations_tested': 70, 'total_train_time': 6.161039113998413, 'longest_train_time': 0.805800199508667}])\n",
    "model_stats.append([{'name': 'AdaBoostClassifier', 'tuned_params': 3, 'variations_tested': 12, 'total_train_time': 2.471341133117676, 'longest_train_time': 0.5814013481140137}, {'name': 'BaggingClassifier', 'tuned_params': 4, 'variations_tested': 240, 'total_train_time': 15.177420139312744, 'longest_train_time': 0.6688508987426758}, {'name': 'BernoulliNB', 'tuned_params': 3, 'variations_tested': 50, 'total_train_time': 0.41498756408691406, 'longest_train_time': 0.013022184371948242}, {'name': 'CalibratedClassifierCV', 'tuned_params': 2, 'variations_tested': 8, 'total_train_time': 4.070744514465332, 'longest_train_time': 2.39577579498291}, {'name': 'CategoricalNB', 'tuned_params': 2, 'variations_tested': 8, 'total_train_time': 0.06620931625366211, 'longest_train_time': 2.39577579498291}, {'name': 'ComplementNB', 'tuned_params': 3, 'variations_tested': 16, 'total_train_time': 0.19800162315368652, 'longest_train_time': 0.017000913619995117}, {'name': 'DecisionTreeClassifier', 'tuned_params': 3, 'variations_tested': 12, 'total_train_time': 0.06698155403137207, 'longest_train_time': 0.010018110275268555}, {'name': 'DummyClassifier', 'tuned_params': 2, 'variations_tested': 1, 'total_train_time': 0.003999948501586914, 'longest_train_time': 0.003999948501586914}, {'name': 'ExtraTreesClassifier', 'tuned_params': 9, 'variations_tested': 5184, 'total_train_time': 1258.402093410492, 'longest_train_time': 0.6468696594238281}, {'name': 'GaussianNB', 'tuned_params': 2, 'variations_tested': 3, 'total_train_time': 0.01800251007080078, 'longest_train_time': 0.006986141204833984}, {'name': 'GaussianProcessClassifier', 'tuned_params': 3, 'variations_tested': 6, 'total_train_time': 13.54332184791565, 'longest_train_time': 7.2592151165008545}, {'name': 'GradientBoostingClassifier', 'tuned_params': 7, 'variations_tested': 4608, 'total_train_time': 320.0416159629822, 'longest_train_time': 8.323050022125244}, {'name': 'HistGradientBoostingClassifier', 'tuned_params': 4, 'variations_tested': 48, 'total_train_time': 89.55422115325928, 'longest_train_time': 3.762437582015991}, {'name': 'KNeighborsClassifier', 'tuned_params': 5, 'variations_tested': 72, 'total_train_time': 3.5163486003875732, 'longest_train_time': 0.10584855079650879}, {'name': 'LabelPropagation', 'tuned_params': 6, 'variations_tested': 240, 'total_train_time': 12.833790063858032, 'longest_train_time': 0.1953725814819336}, {'name': 'LabelSpreading', 'tuned_params': 6, 'variations_tested': 960, 'total_train_time': 64.4865608215332, 'longest_train_time': 0.26899242401123047}, {'name': 'LinearDiscriminantAnalysis', 'tuned_params': 2, 'variations_tested': 6, 'total_train_time': 0.05500197410583496, 'longest_train_time': 0.01599860191345215}, {'name': 'LinearSVC', 'tuned_params': 5, 'variations_tested': 128, 'total_train_time': 1083.744719028473, 'longest_train_time': 27.251423358917236}, {'name': 'LogisticRegression', 'tuned_params': 6, 'variations_tested': 480, 'total_train_time': 334.6323857307434, 'longest_train_time': 2.679772138595581}, {'name': 'LogisticRegressionCV', 'tuned_params': 7, 'variations_tested': 1920, 'total_train_time': 2916.8464612960815, 'longest_train_time': 14.653062343597412}, {'name': 'MLPClassifier', 'tuned_params': 6, 'variations_tested': 1152, 'total_train_time': 4172.146931648254, 'longest_train_time': 84.6691837310791}, {'name': 'MultinomialNB', 'tuned_params': 2, 'variations_tested': 12, 'total_train_time': 0.08304214477539062, 'longest_train_time': 0.00900125503540039}, {'name': 'NearestCentroid', 'tuned_params': 2, 'variations_tested': 15, 'total_train_time': 0.10599994659423828, 'longest_train_time': 0.009999752044677734}, {'name': 'NuSVC', 'tuned_params': 3, 'variations_tested': 80, 'total_train_time': 0.24400115013122559, 'longest_train_time': 0.009999752044677734}, {'name': 'PassiveAggressiveClassifier', 'tuned_params': 3, 'variations_tested': 10, 'total_train_time': 0.1680009365081787, 'longest_train_time': 0.03600335121154785}, {'name': 'Perceptron', 'tuned_params': 3, 'variations_tested': 80, 'total_train_time': 1.257070541381836, 'longest_train_time': 0.031085968017578125}, {'name': 'QuadraticDiscriminantAnalysis', 'tuned_params': 2, 'variations_tested': 15, 'total_train_time': 0.1500229835510254, 'longest_train_time': 0.01300358772277832}, {'name': 'RadiusNeighborsClassifier', 'tuned_params': 6, 'variations_tested': 480, 'total_train_time': 7.737311840057373, 'longest_train_time': 0.04724693298339844}, {'name': 'RandomForestClassifier', 'tuned_params': 7, 'variations_tested': 1152, 'total_train_time': 59.118136167526245, 'longest_train_time': 0.35652804374694824}, {'name': 'RidgeClassifier', 'tuned_params': 3, 'variations_tested': 70, 'total_train_time': 5.590464115142822, 'longest_train_time': 0.7264857292175293}, {'name': 'SGDClassifier', 'tuned_params': 9, 'variations_tested': 69120, 'total_train_time': 2692.593199491501, 'longest_train_time': 0.7622888088226318}, {'name': 'SVC', 'tuned_params': 3, 'variations_tested': 64, 'total_train_time': 17.96438431739807, 'longest_train_time': 3.144374370574951}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script trained a total of 276699 models\n",
      "and failed to train 36204 models\n",
      "out of 312903 possible models\n",
      "with a total training time of 55503.04581273927s,\n",
      "that is, 925.0507635456545 minutes,\n",
      "that is, 15.417512725760908 hours,\n",
      "that is, 0.6423963635733712 days... WOW!\n"
     ]
    }
   ],
   "source": [
    "# global stats of all executions\n",
    "total_global_stats = {\n",
    "    'trained_models': 0, \n",
    "    'failed_models': 0, \n",
    "    'total_models': 0, \n",
    "    'training_time': 0, \n",
    "    'total_training_time': 0\n",
    "    }\n",
    "\n",
    "for stat in global_stats:\n",
    "    total_global_stats[\"trained_models\"] += stat[\"trained_models\"]\n",
    "    total_global_stats[\"failed_models\"] += stat[\"failed_models\"]\n",
    "    total_global_stats[\"total_models\"] += stat[\"total_models\"]\n",
    "    total_global_stats[\"total_training_time\"] += stat[\"total_training_time\"]\n",
    "\n",
    "# print(total_global_stats)\n",
    "print(\"This script trained a total of \" + str(total_global_stats[\"trained_models\"]) + \" models\")\n",
    "print(\"and failed to train \" + str(total_global_stats[\"failed_models\"]) + \" models\")\n",
    "print(\"out of \" + str(total_global_stats[\"total_models\"]) + \" possible models\")\n",
    "print(\"with a total training time of \" + str(total_global_stats[\"total_training_time\"]) + \"s,\")\n",
    "print(\"that is, \" + str(total_global_stats[\"total_training_time\"]/60) + \" minutes,\")\n",
    "print(\"that is, \" + str(total_global_stats[\"total_training_time\"]/3600) + \" hours,\")\n",
    "print(\"that is, \" + str(total_global_stats[\"total_training_time\"]/86400) + \" days... WOW!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stats about different models\n",
    "try:\n",
    "    del total_model_stats\n",
    "except:\n",
    "    total_model_stats = []\n",
    "total_model_stats = []\n",
    "# print(total_model_stats)\n",
    "for stat in model_stats:\n",
    "    for model in stat:\n",
    "        # check if the model exists in the list yet and save the index\n",
    "        _found = False\n",
    "        _key = 0\n",
    "        for m in total_model_stats:\n",
    "            if (\"name\", model[\"name\"]) in m.items():\n",
    "                _found = True\n",
    "                break\n",
    "            _key += 1\n",
    "\n",
    "        #if the model exists, update the stats\n",
    "        if _found:\n",
    "            total_model_stats[_key][\"variations_tested\"] += model[\"variations_tested\"]\n",
    "            total_model_stats[_key][\"total_train_time\"] += model[\"total_train_time\"]\n",
    "            if total_model_stats[_key][\"longest_train_time\"] < model[\"longest_train_time\"]:\n",
    "                total_model_stats[_key][\"longest_train_time\"] = model[\"longest_train_time\"]\n",
    "        else:\n",
    "            total_model_stats.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier had 3 params tuned\n",
      "for a total of 84 variations tested\n",
      "achieveing a total train time for this model type of 16.7s\n",
      "which was 0.03% of the total train time\n",
      "with the longest variation train time of 0.78s\n",
      "\n",
      "BaggingClassifier had 4 params tuned\n",
      "for a total of 1680 variations tested\n",
      "achieveing a total train time for this model type of 133.57s\n",
      "which was 0.24% of the total train time\n",
      "with the longest variation train time of 1.68s\n",
      "\n",
      "BernoulliNB had 3 params tuned\n",
      "for a total of 200 variations tested\n",
      "achieveing a total train time for this model type of 1.96s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.04s\n",
      "\n",
      "CalibratedClassifierCV had 2 params tuned\n",
      "for a total of 32 variations tested\n",
      "achieveing a total train time for this model type of 15.69s\n",
      "which was 0.03% of the total train time\n",
      "with the longest variation train time of 3.46s\n",
      "\n",
      "CategoricalNB had 2 params tuned\n",
      "for a total of 32 variations tested\n",
      "achieveing a total train time for this model type of 0.31s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 3.46s\n",
      "\n",
      "ComplementNB had 3 params tuned\n",
      "for a total of 64 variations tested\n",
      "achieveing a total train time for this model type of 0.65s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.02s\n",
      "\n",
      "DecisionTreeClassifier had 3 params tuned\n",
      "for a total of 48 variations tested\n",
      "achieveing a total train time for this model type of 0.43s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.04s\n",
      "\n",
      "DummyClassifier had 2 params tuned\n",
      "for a total of 4 variations tested\n",
      "achieveing a total train time for this model type of 0.02s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.01s\n",
      "\n",
      "ExtraTreesClassifier had 9 params tuned\n",
      "for a total of 36288 variations tested\n",
      "achieveing a total train time for this model type of 9623.5s\n",
      "which was 17.34% of the total train time\n",
      "with the longest variation train time of 3.07s\n",
      "\n",
      "GaussianNB had 2 params tuned\n",
      "for a total of 21 variations tested\n",
      "achieveing a total train time for this model type of 0.13s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.01s\n",
      "\n",
      "GaussianProcessClassifier had 3 params tuned\n",
      "for a total of 24 variations tested\n",
      "achieveing a total train time for this model type of 39.2s\n",
      "which was 0.07% of the total train time\n",
      "with the longest variation train time of 7.26s\n",
      "\n",
      "GradientBoostingClassifier had 7 params tuned\n",
      "for a total of 29952 variations tested\n",
      "achieveing a total train time for this model type of 1917.98s\n",
      "which was 3.46% of the total train time\n",
      "with the longest variation train time of 8.83s\n",
      "\n",
      "HistGradientBoostingClassifier had 4 params tuned\n",
      "for a total of 336 variations tested\n",
      "achieveing a total train time for this model type of 887.55s\n",
      "which was 1.6% of the total train time\n",
      "with the longest variation train time of 7.8s\n",
      "\n",
      "KNeighborsClassifier had 5 params tuned\n",
      "for a total of 288 variations tested\n",
      "achieveing a total train time for this model type of 14.52s\n",
      "which was 0.03% of the total train time\n",
      "with the longest variation train time of 0.15s\n",
      "\n",
      "LabelPropagation had 6 params tuned\n",
      "for a total of 1200 variations tested\n",
      "achieveing a total train time for this model type of 66.54s\n",
      "which was 0.12% of the total train time\n",
      "with the longest variation train time of 0.25s\n",
      "\n",
      "LabelSpreading had 6 params tuned\n",
      "for a total of 3840 variations tested\n",
      "achieveing a total train time for this model type of 260.08s\n",
      "which was 0.47% of the total train time\n",
      "with the longest variation train time of 0.37s\n",
      "\n",
      "LinearDiscriminantAnalysis had 2 params tuned\n",
      "for a total of 24 variations tested\n",
      "achieveing a total train time for this model type of 0.27s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.03s\n",
      "\n",
      "LinearSVC had 5 params tuned\n",
      "for a total of 640 variations tested\n",
      "achieveing a total train time for this model type of 5482.18s\n",
      "which was 9.88% of the total train time\n",
      "with the longest variation train time of 29.74s\n",
      "\n",
      "LogisticRegression had 6 params tuned\n",
      "for a total of 2080 variations tested\n",
      "achieveing a total train time for this model type of 1508.31s\n",
      "which was 2.72% of the total train time\n",
      "with the longest variation train time of 4.07s\n",
      "\n",
      "LogisticRegressionCV had 7 params tuned\n",
      "for a total of 9600 variations tested\n",
      "achieveing a total train time for this model type of 14619.9s\n",
      "which was 26.34% of the total train time\n",
      "with the longest variation train time of 28.33s\n",
      "\n",
      "MLPClassifier had 6 params tuned\n",
      "for a total of 5760 variations tested\n",
      "achieveing a total train time for this model type of 26919.18s\n",
      "which was 48.5% of the total train time\n",
      "with the longest variation train time of 263.91s\n",
      "\n",
      "MultinomialNB had 2 params tuned\n",
      "for a total of 48 variations tested\n",
      "achieveing a total train time for this model type of 0.36s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.01s\n",
      "\n",
      "NearestCentroid had 2 params tuned\n",
      "for a total of 60 variations tested\n",
      "achieveing a total train time for this model type of 0.52s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.02s\n",
      "\n",
      "NuSVC had 3 params tuned\n",
      "for a total of 320 variations tested\n",
      "achieveing a total train time for this model type of 0.83s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.02s\n",
      "\n",
      "PassiveAggressiveClassifier had 3 params tuned\n",
      "for a total of 40 variations tested\n",
      "achieveing a total train time for this model type of 0.59s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.1s\n",
      "\n",
      "Perceptron had 3 params tuned\n",
      "for a total of 320 variations tested\n",
      "achieveing a total train time for this model type of 4.88s\n",
      "which was 0.01% of the total train time\n",
      "with the longest variation train time of 0.06s\n",
      "\n",
      "QuadraticDiscriminantAnalysis had 2 params tuned\n",
      "for a total of 105 variations tested\n",
      "achieveing a total train time for this model type of 1.35s\n",
      "which was 0.0% of the total train time\n",
      "with the longest variation train time of 0.04s\n",
      "\n",
      "RadiusNeighborsClassifier had 6 params tuned\n",
      "for a total of 1920 variations tested\n",
      "achieveing a total train time for this model type of 34.63s\n",
      "which was 0.06% of the total train time\n",
      "with the longest variation train time of 0.08s\n",
      "\n",
      "RandomForestClassifier had 7 params tuned\n",
      "for a total of 8064 variations tested\n",
      "achieveing a total train time for this model type of 607.59s\n",
      "which was 1.09% of the total train time\n",
      "with the longest variation train time of 1.01s\n",
      "\n",
      "RidgeClassifier had 3 params tuned\n",
      "for a total of 280 variations tested\n",
      "achieveing a total train time for this model type of 21.57s\n",
      "which was 0.04% of the total train time\n",
      "with the longest variation train time of 0.81s\n",
      "\n",
      "SGDClassifier had 9 params tuned\n",
      "for a total of 207360 variations tested\n",
      "achieveing a total train time for this model type of 7891.09s\n",
      "which was 14.22% of the total train time\n",
      "with the longest variation train time of 2.19s\n",
      "\n",
      "SVC had 3 params tuned\n",
      "for a total of 256 variations tested\n",
      "achieveing a total train time for this model type of 3533.81s\n",
      "which was 6.37% of the total train time\n",
      "with the longest variation train time of 197.81s\n",
      "\n",
      "132.61593784140243\n"
     ]
    }
   ],
   "source": [
    "total_percentage = 0\n",
    "\n",
    "for model in total_model_stats:\n",
    "    total_percentage += 100*model[\"total_train_time\"]/total_global_stats[\"total_training_time\"]\n",
    "    print(model[\"name\"] + \" had \" + str(model[\"tuned_params\"]) + \" params tuned\")\n",
    "    print(\"for a total of \" + str(model[\"variations_tested\"]) + \" variations tested\")\n",
    "    print(\"achieveing a total train time for this model type of \" + str(round(model[\"total_train_time\"], 2)) + \"s\")\n",
    "    print(\"which was \" + str(round(100*model[\"total_train_time\"]/total_global_stats[\"total_training_time\"], 2)) + \"%\" + \" of the total train time\")\n",
    "    print(\"with the longest variation train time of \" + str(round(model[\"longest_train_time\"], 2)) + \"s\\n\")\n",
    "print(total_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6a3af7cc3dd87b1c9aec9c941132942751acefef11dd989f38e57e7ca82ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
